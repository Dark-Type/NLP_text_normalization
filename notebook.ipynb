{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-31T13:39:48.480290Z",
     "start_time": "2025-05-31T13:39:48.444497Z"
    }
   },
   "source": [
    "import re\n",
    "import logging\n",
    "from typing import Dict, List, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('./data/log_file.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class My_TextNormalization_Model:\n",
    "    \"\"\"\n",
    "    Improved rule-based Russian text normalization model for TTS systems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the rule-based text normalization model.\"\"\"\n",
    "        logger.info(\"Initializing Improved Russian Text Normalization Model\")\n",
    "\n",
    "        self.numbers = {\n",
    "            '0': 'ноль', '1': 'один', '2': 'два', '3': 'три', '4': 'четыре',\n",
    "            '5': 'пять', '6': 'шесть', '7': 'семь', '8': 'восемь', '9': 'девять',\n",
    "            '10': 'десять', '11': 'одиннадцать', '12': 'двенадцать',\n",
    "            '13': 'тринадцать', '14': 'четырнадцать', '15': 'пятнадцать',\n",
    "            '16': 'шестнадцать', '17': 'семнадцать', '18': 'восемнадцать',\n",
    "            '19': 'девятнадцать', '20': 'двадцать', '30': 'тридцать',\n",
    "            '40': 'сорок', '50': 'пятьдесят', '60': 'шестьдесят',\n",
    "            '70': 'семьдесят', '80': 'восемьдесят', '90': 'девяносто'\n",
    "        }\n",
    "\n",
    "        self.hundreds = {\n",
    "            '100': 'сто', '200': 'двести', '300': 'триста', '400': 'четыреста',\n",
    "            '500': 'пятьсот', '600': 'шестьсот', '700': 'семьсот',\n",
    "            '800': 'восемьсот', '900': 'девятьсот'\n",
    "        }\n",
    "\n",
    "        self.ordinals = {\n",
    "            '1': 'первое', '2': 'второе', '3': 'третье', '4': 'четвертое',\n",
    "            '5': 'пятое', '6': 'шестое', '7': 'седьмое', '8': 'восьмое',\n",
    "            '9': 'девятое', '10': 'десятое', '11': 'одиннадцатое',\n",
    "            '12': 'двенадцатое', '13': 'тринадцатое', '14': 'четырнадцатое',\n",
    "            '15': 'пятнадцатое', '16': 'шестнадцатое', '17': 'семнадцатое',\n",
    "            '18': 'восемнадцатое', '19': 'девятнадцатое', '20': 'двадцатое',\n",
    "            '21': 'двадцать первое', '22': 'двадцать второе', '23': 'двадцать третье',\n",
    "            '24': 'двадцать четвертое', '25': 'двадцать пятое', '26': 'двадцать шестое',\n",
    "            '27': 'двадцать седьмое', '28': 'двадцать восьмое', '29': 'двадцать девятое',\n",
    "            '30': 'тридцатое', '31': 'тридцать первое'\n",
    "        }\n",
    "\n",
    "        self.months = {\n",
    "            '1': 'января', '2': 'февраля', '3': 'марта', '4': 'апреля',\n",
    "            '5': 'мая', '6': 'июня', '7': 'июля', '8': 'августа',\n",
    "            '9': 'сентября', '10': 'октября', '11': 'ноября', '12': 'декабря',\n",
    "            '01': 'января', '02': 'февраля', '03': 'марта', '04': 'апреля',\n",
    "            '05': 'мая', '06': 'июня', '07': 'июля', '08': 'августа',\n",
    "            '09': 'сентября'\n",
    "        }\n",
    "\n",
    "        self.roman_numerals = {\n",
    "            'I': 'первый', 'II': 'второй', 'III': 'третий', 'IV': 'четвертый',\n",
    "            'V': 'пятый', 'VI': 'шестой', 'VII': 'седьмой', 'VIII': 'восьмой',\n",
    "            'IX': 'девятый', 'X': 'десятый', 'XI': 'одиннадцатый',\n",
    "            'XII': 'двенадцатый', 'XIII': 'тринадцатый', 'XIV': 'четырнадцатый',\n",
    "            'XV': 'пятнадцатый', 'XVI': 'шестнадцатый', 'XVII': 'семнадцатый',\n",
    "            'XVIII': 'восемнадцатый', 'XIX': 'девятнадцатый', 'XX': 'двадцатый'\n",
    "        }\n",
    "\n",
    "        logger.info(\"Improved Russian Text Normalization Model initialized successfully\")\n",
    "\n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Main method to normalize Russian text for TTS.\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            logger.warning(\"Empty or invalid input text received\")\n",
    "            return \"\"\n",
    "\n",
    "        logger.info(f\"Normalizing text: {text[:50]}...\")\n",
    "\n",
    "        try:\n",
    "            normalized_text = self._preprocess_text(text)\n",
    "\n",
    "            normalized_text = self._normalize_dates(normalized_text)\n",
    "            normalized_text = self._normalize_time(normalized_text)\n",
    "            normalized_text = self._normalize_currency(normalized_text)\n",
    "            normalized_text = self._normalize_measurements(normalized_text)\n",
    "            normalized_text = self._normalize_percentages(normalized_text)\n",
    "            normalized_text = self._normalize_phone_numbers(normalized_text)\n",
    "            normalized_text = self._normalize_urls_emails(normalized_text)\n",
    "            normalized_text = self._normalize_abbreviations(normalized_text)\n",
    "            normalized_text = self._normalize_roman_numerals(normalized_text)\n",
    "            normalized_text = self._normalize_numbers(normalized_text)  # Numbers last to avoid conflicts\n",
    "            normalized_text = self._normalize_punctuation(normalized_text)\n",
    "\n",
    "            normalized_text = self._postprocess_text(normalized_text)\n",
    "\n",
    "            logger.info(f\"Text normalization completed successfully\")\n",
    "            return normalized_text\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during text normalization: {str(e)}\")\n",
    "            return text\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess text by cleaning and standardizing format.\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        text = text.replace('—', '-').replace('–', '-')\n",
    "        text = text.replace('\"', '\"').replace('\"', '\"')\n",
    "        text = text.replace(''', \"'\").replace(''', \"'\")\n",
    "        return text\n",
    "\n",
    "    def _normalize_dates(self, text: str) -> str:\n",
    "        \"\"\"Normalize dates in various formats.\"\"\"\n",
    "        def convert_date(match):\n",
    "            day, month, year = match.groups()\n",
    "            day_word = self.ordinals.get(str(int(day)), f\"{day}-е\")\n",
    "            month_word = self.months.get(str(int(month)), month)\n",
    "            year_word = self._convert_year(year)\n",
    "            return f\"{day_word} {month_word} {year_word} года\"\n",
    "\n",
    "        text = re.sub(r'\\b(\\d{1,2})[./](\\d{1,2})[./](\\d{4})\\b', convert_date, text)\n",
    "\n",
    "        def convert_short_date(match):\n",
    "            day, month = match.groups()\n",
    "            day_word = self.ordinals.get(str(int(day)), f\"{day}-е\")\n",
    "            month_word = self.months.get(str(int(month)), month)\n",
    "            return f\"{day_word} {month_word}\"\n",
    "\n",
    "        text = re.sub(r'\\b(\\d{1,2})[./](\\d{1,2})\\b(?!/)', convert_short_date, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_time(self, text: str) -> str:\n",
    "        \"\"\"Normalize time expressions - FIXED VERSION.\"\"\"\n",
    "        def convert_time(match):\n",
    "            hours, minutes = match.groups()\n",
    "            hour_int = int(hours)\n",
    "            minute_int = int(minutes)\n",
    "\n",
    "            hour_word = self._number_to_words(hour_int)\n",
    "\n",
    "            if minute_int == 0:\n",
    "                return f\"{hour_word} часов\"\n",
    "            else:\n",
    "                minute_word = self._number_to_words(minute_int)\n",
    "                return f\"{hour_word} часов {minute_word} минут\"\n",
    "\n",
    "        text = re.sub(r'\\b(\\d{1,2}):(\\d{2})\\b', convert_time, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_currency(self, text: str) -> str:\n",
    "        \"\"\"Normalize currency expressions - FIXED VERSION.\"\"\"\n",
    "        def convert_currency(match):\n",
    "            amount, currency = match.groups()\n",
    "            amount_int = int(amount)\n",
    "            amount_word = self._number_to_words(amount_int)\n",
    "\n",
    "            if currency == '₽' or currency == 'руб':\n",
    "                if amount_int == 1:\n",
    "                    currency_word = 'рубль'\n",
    "                elif 2 <= amount_int <= 4:\n",
    "                    currency_word = 'рубля'\n",
    "                else:\n",
    "                    currency_word = 'рублей'\n",
    "            elif currency == '$':\n",
    "                if amount_int == 1:\n",
    "                    currency_word = 'доллар'\n",
    "                elif 2 <= amount_int <= 4:\n",
    "                    currency_word = 'доллара'\n",
    "                else:\n",
    "                    currency_word = 'долларов'\n",
    "            elif currency == '€':\n",
    "                currency_word = 'евро'\n",
    "            else:\n",
    "                currency_word = currency\n",
    "\n",
    "            return f\"{amount_word} {currency_word}\"\n",
    "\n",
    "        text = re.sub(r'(\\d+)\\s*([₽$€]|руб\\.?)', convert_currency, text)\n",
    "\n",
    "        def convert_decimal_currency(match):\n",
    "            amount, currency = match.groups()\n",
    "            amount_word = self._convert_decimal_number(amount)\n",
    "            currency_word = self._get_currency_word(currency)\n",
    "            return f\"{amount_word} {currency_word}\"\n",
    "\n",
    "        text = re.sub(r'(\\d+[.,]\\d+)\\s*([₽$€]|руб\\.?)', convert_decimal_currency, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_measurements(self, text: str) -> str:\n",
    "        \"\"\"Normalize measurement units - FIXED VERSION.\"\"\"\n",
    "        def convert_measurement(match):\n",
    "            amount, unit = match.groups()\n",
    "\n",
    "            if '.' in amount or ',' in amount:\n",
    "                amount_word = self._convert_decimal_number(amount)\n",
    "            else:\n",
    "                amount_word = self._number_to_words(int(amount))\n",
    "\n",
    "            unit_mappings = {\n",
    "                'кг': 'килограммов',\n",
    "                'г': 'граммов',\n",
    "                'км': 'километров',\n",
    "                'м': 'метров',\n",
    "                'см': 'сантиметров',\n",
    "                'мм': 'миллиметров',\n",
    "                'л': 'литров',\n",
    "                'мл': 'миллилитров',\n",
    "                '°C': 'градусов цельсия',\n",
    "                '°': 'градусов'\n",
    "            }\n",
    "\n",
    "            unit_word = unit_mappings.get(unit, unit)\n",
    "            return f\"{amount_word} {unit_word}\"\n",
    "\n",
    "        text = re.sub(r'(\\d+(?:[.,]\\d+)?)\\s*(кг|г|км|м|см|мм|л|мл|°C|°)', convert_measurement, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_percentages(self, text: str) -> str:\n",
    "        \"\"\"Normalize percentage expressions - FIXED VERSION.\"\"\"\n",
    "        def convert_percentage(match):\n",
    "            amount = match.group(1)\n",
    "            if '.' in amount or ',' in amount:\n",
    "                amount_word = self._convert_decimal_number(amount)\n",
    "            else:\n",
    "                amount_word = self._number_to_words(int(amount))\n",
    "            return f\"{amount_word} процентов\"\n",
    "\n",
    "        text = re.sub(r'(\\d+(?:[.,]\\d+)?)%', convert_percentage, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_phone_numbers(self, text: str) -> str:\n",
    "        \"\"\"Normalize phone numbers - FIXED VERSION.\"\"\"\n",
    "        def convert_phone(match):\n",
    "            phone = match.group()\n",
    "            # Extract just the digits\n",
    "            digits = re.sub(r'[^\\d]', '', phone)\n",
    "\n",
    "            # Convert each digit individually for phone numbers\n",
    "            spoken_digits = []\n",
    "            for digit in digits:\n",
    "                spoken_digits.append(self.numbers.get(digit, digit))\n",
    "\n",
    "            return ' '.join(spoken_digits)\n",
    "\n",
    "        phone_patterns = [\n",
    "            r'\\+7\\s*\\(\\d{3}\\)\\s*\\d{3}-\\d{2}-\\d{2}',\n",
    "            r'\\+7\\s*\\d{10}',\n",
    "            r'8\\s*\\(\\d{3}\\)\\s*\\d{3}-\\d{2}-\\d{2}',\n",
    "            r'8\\s*\\d{10}'\n",
    "        ]\n",
    "\n",
    "        for pattern in phone_patterns:\n",
    "            text = re.sub(pattern, convert_phone, text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _normalize_urls_emails(self, text: str) -> str:\n",
    "        \"\"\"Normalize URLs and email addresses.\"\"\"\n",
    "        text = re.sub(r'https?://[^\\s]+', 'ссылка', text)\n",
    "        text = re.sub(r'www\\.[^\\s]+', 'веб сайт', text)\n",
    "        text = re.sub(r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b', 'электронная почта', text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_abbreviations(self, text: str) -> str:\n",
    "        \"\"\"Normalize common abbreviations.\"\"\"\n",
    "        abbreviations = {\n",
    "            'т.е.': 'то есть', 'и т.д.': 'и так далее', 'и т.п.': 'и тому подобное',\n",
    "            'т.к.': 'так как', 'т.н.': 'так называемый', 'см.': 'смотрите',\n",
    "            'стр.': 'страница', 'гл.': 'глава', 'рис.': 'рисунок',\n",
    "            'табл.': 'таблица', 'г.': 'год', 'гг.': 'годы', 'в.': 'век',\n",
    "            'вв.': 'века', 'др.': 'другие', 'пр.': 'прочие',\n",
    "            'напр.': 'например', 'англ.': 'английский', 'рус.': 'русский'\n",
    "        }\n",
    "\n",
    "        for abbr, expansion in abbreviations.items():\n",
    "            text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', expansion, text, flags=re.IGNORECASE)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _normalize_roman_numerals(self, text: str) -> str:\n",
    "        \"\"\"Normalize Roman numerals.\"\"\"\n",
    "        def convert_roman(match):\n",
    "            roman = match.group()\n",
    "            return self.roman_numerals.get(roman, roman)\n",
    "\n",
    "        text = re.sub(r'\\b(I{1,3}|IV|V|VI{0,3}|IX|X{1,2}|XI{0,3}|XIV|XV|XVI{0,3}|XIX|XX)\\b', convert_roman, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_numbers(self, text: str) -> str:\n",
    "        \"\"\"Normalize standalone numbers to words.\"\"\"\n",
    "        def convert_number(match):\n",
    "            number = match.group()\n",
    "            try:\n",
    "                num = int(number)\n",
    "                return self._number_to_words(num)\n",
    "            except ValueError:\n",
    "                return number\n",
    "\n",
    "        text = re.sub(r'\\b\\d{1,4}\\b', convert_number, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_punctuation(self, text: str) -> str:\n",
    "        \"\"\"Handle punctuation marks.\"\"\"\n",
    "        text = text.replace('&', ' и ')\n",
    "        text = text.replace('+', ' плюс ')\n",
    "        text = text.replace('=', ' равно ')\n",
    "        text = text.replace('№', 'номер ')\n",
    "\n",
    "        text = re.sub(r'[()[\\]{}]', '', text)\n",
    "        text = re.sub(r'[.!?]+', '.', text)\n",
    "        text = re.sub(r'[,;:]+', ',', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _postprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Final cleanup of normalized text.\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        text = re.sub(r'[.!?,;:]+$', '', text)\n",
    "        text = text.lower()\n",
    "        return text\n",
    "\n",
    "    def _number_to_words(self, num: int) -> str:\n",
    "        \"\"\"Convert numbers to Russian words - IMPROVED VERSION.\"\"\"\n",
    "        if num == 0:\n",
    "            return 'ноль'\n",
    "\n",
    "        if 1 <= num <= 19:\n",
    "            return self.numbers[str(num)]\n",
    "        elif 20 <= num <= 99:\n",
    "            tens = (num // 10) * 10\n",
    "            units = num % 10\n",
    "            if units == 0:\n",
    "                return self.numbers[str(tens)]\n",
    "            else:\n",
    "                return f\"{self.numbers[str(tens)]} {self.numbers[str(units)]}\"\n",
    "        elif 100 <= num <= 999:\n",
    "            hundreds = (num // 100) * 100\n",
    "            remainder = num % 100\n",
    "            result = self.hundreds[str(hundreds)]\n",
    "            if remainder > 0:\n",
    "                result += f\" {self._number_to_words(remainder)}\"\n",
    "            return result\n",
    "        elif 1000 <= num <= 9999:\n",
    "            thousands = num // 1000\n",
    "            remainder = num % 1000\n",
    "\n",
    "            if thousands == 1:\n",
    "                result = \"тысяча\"\n",
    "            elif 2 <= thousands <= 4:\n",
    "                result = f\"{self.numbers[str(thousands)]} тысячи\"\n",
    "            else:\n",
    "                result = f\"{self.numbers[str(thousands)]} тысяч\"\n",
    "\n",
    "            if remainder > 0:\n",
    "                result += f\" {self._number_to_words(remainder)}\"\n",
    "            return result\n",
    "        else:\n",
    "            return str(num)\n",
    "\n",
    "    def _convert_decimal_number(self, number_str: str) -> str:\n",
    "        \"\"\"Convert decimal numbers to words.\"\"\"\n",
    "        number_str = number_str.replace(',', '.')\n",
    "\n",
    "        try:\n",
    "            if '.' in number_str:\n",
    "                integer_part, decimal_part = number_str.split('.')\n",
    "                integer_word = self._number_to_words(int(integer_part))\n",
    "                decimal_digits = ' '.join([self.numbers.get(digit, digit) for digit in decimal_part])\n",
    "                return f\"{integer_word} целых {decimal_digits}\"\n",
    "            else:\n",
    "                return self._number_to_words(int(number_str))\n",
    "        except ValueError:\n",
    "            return number_str\n",
    "\n",
    "    def _convert_year(self, year: str) -> str:\n",
    "        \"\"\"Convert year to spoken form.\"\"\"\n",
    "        try:\n",
    "            year_int = int(year)\n",
    "            if 1000 <= year_int <= 2099:\n",
    "                return self._number_to_words(year_int)\n",
    "            else:\n",
    "                return year\n",
    "        except ValueError:\n",
    "            return year\n",
    "\n",
    "    def _get_currency_word(self, currency: str) -> str:\n",
    "        \"\"\"Get proper currency word.\"\"\"\n",
    "        currency_map = {\n",
    "            '₽': 'рублей', 'руб': 'рублей', '$': 'долларов', '€': 'евро'\n",
    "        }\n",
    "        return currency_map.get(currency, currency)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = My_TextNormalization_Model()\n",
    "\n",
    "    test_texts = [\n",
    "        \"Сегодня 15.03.2024 в 14:30 температура была 25°C.\",\n",
    "        \"Цена составляет 1500₽ за 2.5кг.\",\n",
    "        \"Позвоните по номеру +7(495)123-45-67.\",\n",
    "        \"Эффективность увеличилась на 15%.\",\n",
    "        \"В XX веке произошло много изменений.\",\n",
    "        \"Встреча назначена на 10:00.\"\n",
    "    ]\n",
    "\n",
    "    print(\"IMPROVED RULE-BASED MODEL RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for text in test_texts:\n",
    "        normalized = model.normalize_text(text)\n",
    "        print(f\"Original: {text}\")\n",
    "        print(f\"Normalized: {normalized}\")\n",
    "        print(\"-\" * 50)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 20:39:48,473 - __main__ - INFO - Initializing Improved Russian Text Normalization Model\n",
      "2025-05-31 20:39:48,473 - __main__ - INFO - Improved Russian Text Normalization Model initialized successfully\n",
      "2025-05-31 20:39:48,474 - __main__ - INFO - Normalizing text: Сегодня 15.03.2024 в 14:30 температура была 25°C....\n",
      "2025-05-31 20:39:48,475 - __main__ - INFO - Text normalization completed successfully\n",
      "2025-05-31 20:39:48,476 - __main__ - INFO - Normalizing text: Цена составляет 1500₽ за 2.5кг....\n",
      "2025-05-31 20:39:48,476 - __main__ - INFO - Text normalization completed successfully\n",
      "2025-05-31 20:39:48,476 - __main__ - INFO - Normalizing text: Позвоните по номеру +7(495)123-45-67....\n",
      "2025-05-31 20:39:48,477 - __main__ - INFO - Text normalization completed successfully\n",
      "2025-05-31 20:39:48,477 - __main__ - INFO - Normalizing text: Эффективность увеличилась на 15%....\n",
      "2025-05-31 20:39:48,477 - __main__ - INFO - Text normalization completed successfully\n",
      "2025-05-31 20:39:48,478 - __main__ - INFO - Normalizing text: В XX веке произошло много изменений....\n",
      "2025-05-31 20:39:48,478 - __main__ - INFO - Text normalization completed successfully\n",
      "2025-05-31 20:39:48,478 - __main__ - INFO - Normalizing text: Встреча назначена на 10:00....\n",
      "2025-05-31 20:39:48,478 - __main__ - INFO - Text normalization completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPROVED RULE-BASED MODEL RESULTS:\n",
      "============================================================\n",
      "Original: Сегодня 15.03.2024 в 14:30 температура была 25°C.\n",
      "Normalized: сегодня пятнадцатое марта два тысячи двадцать четыре года в четырнадцать часов тридцать минут температура была двадцать пять градусов цельсия\n",
      "--------------------------------------------------\n",
      "Original: Цена составляет 1500₽ за 2.5кг.\n",
      "Normalized: цена составляет тысяча пятьсот рублей за два целых пять килограммов\n",
      "--------------------------------------------------\n",
      "Original: Позвоните по номеру +7(495)123-45-67.\n",
      "Normalized: позвоните по номеру семь четыре девять пять один два три четыре пять шесть семь\n",
      "--------------------------------------------------\n",
      "Original: Эффективность увеличилась на 15%.\n",
      "Normalized: эффективность увеличилась на пятнадцать процентов\n",
      "--------------------------------------------------\n",
      "Original: В XX веке произошло много изменений.\n",
      "Normalized: в двадцатый веке произошло много изменений\n",
      "--------------------------------------------------\n",
      "Original: Встреча назначена на 10:00.\n",
      "Normalized: встреча назначена на десять часов\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T16:21:09.798981Z",
     "start_time": "2025-05-31T16:21:04.708440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('./data/log_file.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration optimized for small dataset (13k samples).\"\"\"\n",
    "    model_name: str = \"ai-forever/ruT5-base\"\n",
    "    max_source_length: int = 128\n",
    "    max_target_length: int = 128\n",
    "\n",
    "    batch_size: int = 8\n",
    "    learning_rate: float = 1e-4\n",
    "    num_epochs: int = 1\n",
    "    warmup_steps: int = 100\n",
    "    weight_decay: float = 0.05\n",
    "    gradient_clip_val: float = 0.5\n",
    "\n",
    "    save_steps: int = 200\n",
    "    eval_steps: int = 100\n",
    "\n",
    "    # Regularization settings\n",
    "    dropout_rate: float = 0.3\n",
    "    use_early_stopping: bool = True\n",
    "    patience: int = 5\n",
    "\n",
    "    sample_size: int = 15000\n",
    "    stratify_by_class: bool = True\n",
    "    stratify_by_length: bool = True\n",
    "\n",
    "    # Performance\n",
    "    dataloader_num_workers: int = 4\n",
    "    mixed_precision: bool = True\n",
    "    compile_model: bool = False\n",
    "    use_wandb: bool = True\n",
    "    seed: int = 42\n",
    "\n",
    "class RobustRussianTextCleaner:\n",
    "    \"\"\"Improved text cleaner based on your implementation.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        self.russian_letters = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ')\n",
    "\n",
    "        self.valid_chars = (\n",
    "            self.russian_letters |\n",
    "            set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789') |\n",
    "            set(' .,!?-()[]{}\";:\\'\"/@#$%^&*+=<>~`|\\\\€$₽°')\n",
    "        )\n",
    "\n",
    "    def safe_load_csv(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Safely load CSV with encoding issues handling.\"\"\"\n",
    "        encodings_to_try = ['utf-8', 'utf-8-sig', 'cp1251', 'iso-8859-1', 'latin1']\n",
    "\n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                self.logger.info(f\"Trying to load with encoding: {encoding}\")\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "                self.logger.info(f\"Successfully loaded with {encoding}\")\n",
    "                return df\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Failed with {encoding}: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            self.logger.info(\"Loading with error handling (replacing bad characters)\")\n",
    "            df = pd.read_csv(file_path, encoding='utf-8', encoding_errors='replace')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"All encoding attempts failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def clean_dataset(self, df: pd.DataFrame, aggressive_cleaning: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean Russian text normalization dataset with robust character handling.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with columns ['sentence_id', 'token_id', 'class', 'before', 'after']\n",
    "            aggressive_cleaning: If True, remove more suspicious data (set to False by default)\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting robust cleaning of {len(df)} rows\")\n",
    "        original_size = len(df)\n",
    "\n",
    "        df = self._handle_basic_issues(df)\n",
    "        self._log_step(\"Basic data handling\", original_size, len(df))\n",
    "\n",
    "        df = self._fix_character_encoding(df)\n",
    "        self._log_step(\"Character encoding fix\", original_size, len(df))\n",
    "\n",
    "        df = self._clean_problematic_characters(df, gentle=True)\n",
    "        self._log_step(\"Problematic character removal\", original_size, len(df))\n",
    "\n",
    "        if aggressive_cleaning:\n",
    "            df = self._validate_normalizations(df)\n",
    "            self._log_step(\"Normalization validation\", original_size, len(df))\n",
    "\n",
    "        df = self._handle_clear_duplicates(df)\n",
    "        self._log_step(\"Clear duplicate handling\", original_size, len(df))\n",
    "\n",
    "        df = self._minimal_final_cleanup(df)\n",
    "        self._log_step(\"Minimal final cleanup\", original_size, len(df))\n",
    "\n",
    "        cleaned_size = len(df)\n",
    "        removal_percentage = ((original_size - cleaned_size) / original_size) * 100\n",
    "        self.logger.info(f\"Gentle cleaning complete: {original_size} -> {cleaned_size} rows \"\n",
    "                        f\"({removal_percentage:.1f}% removed)\")\n",
    "\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def _handle_basic_issues(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handle basic data integrity issues.\"\"\"\n",
    "        required_cols = ['before', 'after']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "        df['before'] = df['before'].astype(str).replace('nan', '')\n",
    "        df['after'] = df['after'].astype(str).replace('nan', '')\n",
    "\n",
    "        initial_len = len(df)\n",
    "        df = df[(df['before'].str.strip() != '') & (df['after'].str.strip() != '')]\n",
    "        self.logger.info(f\"Removed {initial_len - len(df)} clearly empty rows\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _fix_character_encoding(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Fix character encoding issues.\"\"\"\n",
    "        def fix_encoding(text):\n",
    "            if pd.isna(text):\n",
    "                return \"\"\n",
    "\n",
    "            text = str(text)\n",
    "\n",
    "            try:\n",
    "                text = unicodedata.normalize('NFKC', text)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            encoding_fixes = {\n",
    "                'Ã¡': 'а', 'Ã ': 'а', 'Ã«': 'е', 'Ã¬': 'и', 'Ã®': 'о', 'Ã³': 'у',\n",
    "                'â€œ': '\"', 'â€': '\"', 'â€™': \"'\", 'â€\"': '–', 'â€\"': '—'\n",
    "            }\n",
    "\n",
    "            for wrong, correct in encoding_fixes.items():\n",
    "                text = text.replace(wrong, correct)\n",
    "\n",
    "            return text\n",
    "\n",
    "        df['before'] = df['before'].apply(fix_encoding)\n",
    "        df['after'] = df['after'].apply(fix_encoding)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _clean_problematic_characters(self, df: pd.DataFrame, gentle: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Remove problematic characters with gentle approach.\"\"\"\n",
    "        def clean_text(text, gentle_mode=True):\n",
    "            if pd.isna(text):\n",
    "                return \"\"\n",
    "\n",
    "            text = str(text)\n",
    "\n",
    "            if gentle_mode:\n",
    "                problematic_chars = set(['', '', '', '﻿', '\\x00', '\\x01', '\\x02', '\\x03'])\n",
    "                text = ''.join(char for char in text if char not in problematic_chars)\n",
    "\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            else:\n",
    "                text = ''.join(char for char in text if char in self.valid_chars or char.isspace())\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            return text\n",
    "\n",
    "        initial_len = len(df)\n",
    "\n",
    "        df['before'] = df['before'].apply(lambda x: clean_text(x, gentle))\n",
    "        df['after'] = df['after'].apply(lambda x: clean_text(x, gentle))\n",
    "\n",
    "        df = df[(df['before'].str.strip() != '') & (df['after'].str.strip() != '')]\n",
    "\n",
    "        self.logger.info(f\"Character cleaning removed {initial_len - len(df)} rows\")\n",
    "        return df\n",
    "\n",
    "    def _validate_normalizations(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Validate normalizations (only if aggressive cleaning is enabled).\"\"\"\n",
    "        initial_len = len(df)\n",
    "\n",
    "        def is_valid_normalization(before, after):\n",
    "            if len(after) > len(before) * 5:\n",
    "                return False\n",
    "\n",
    "            if len(after) < len(before) * 0.1 and len(before) > 10:\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        mask = df.apply(lambda row: is_valid_normalization(row['before'], row['after']), axis=1)\n",
    "        df = df[mask]\n",
    "\n",
    "        self.logger.info(f\"Normalization validation removed {initial_len - len(df)} rows\")\n",
    "        return df\n",
    "\n",
    "    def _handle_clear_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handle only clear duplicates.\"\"\"\n",
    "        initial_len = len(df)\n",
    "\n",
    "        df = df.drop_duplicates(subset=['before', 'after'])\n",
    "\n",
    "        self.logger.info(f\"Duplicate removal: {initial_len - len(df)} exact duplicates removed\")\n",
    "        return df\n",
    "\n",
    "    def _minimal_final_cleanup(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Minimal final cleanup.\"\"\"\n",
    "        initial_len = len(df)\n",
    "\n",
    "        df = df[df['before'].str.len() <= 500]\n",
    "        df = df[df['after'].str.len() <= 600]\n",
    "\n",
    "        self.logger.info(f\"Final cleanup removed {initial_len - len(df)} overly long texts\")\n",
    "        return df\n",
    "\n",
    "    def _log_step(self, step_name: str, original_size: int, current_size: int):\n",
    "        \"\"\"Log cleaning step results.\"\"\"\n",
    "        removed = original_size - current_size\n",
    "        percentage = (removed / original_size) * 100 if original_size > 0 else 0\n",
    "        self.logger.info(f\"{step_name}: {original_size} -> {current_size} \"\n",
    "                        f\"({removed} removed, {percentage:.1f}%)\")\n",
    "\n",
    "class StratifiedDataSampler:\n",
    "    \"\"\"Intelligent sampling to get a representative subset of the large dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def sample_dataset(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a stratified sample of the dataset for fast training.\n",
    "\n",
    "        Args:\n",
    "            df: Full cleaned dataset\n",
    "\n",
    "        Returns:\n",
    "            Stratified sample for training\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Creating stratified sample of {self.config.sample_size} from {len(df)} rows\")\n",
    "\n",
    "        if len(df) <= self.config.sample_size:\n",
    "            self.logger.info(\"Dataset smaller than sample size, using full dataset\")\n",
    "            return df\n",
    "\n",
    "        if 'class' in df.columns and self.config.stratify_by_class:\n",
    "            df_sampled = self._stratify_by_class(df)\n",
    "        else:\n",
    "            df_sampled = df.copy()\n",
    "\n",
    "        if self.config.stratify_by_length:\n",
    "            df_sampled = self._stratify_by_length(df_sampled)\n",
    "\n",
    "        if len(df_sampled) > self.config.sample_size:\n",
    "            df_sampled = df_sampled.sample(n=self.config.sample_size, random_state=42)\n",
    "\n",
    "        self.logger.info(f\"Final sample size: {len(df_sampled)}\")\n",
    "        return df_sampled.reset_index(drop=True)\n",
    "\n",
    "    def _stratify_by_class(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Stratify sampling by normalization class.\"\"\"\n",
    "        class_counts = df['class'].value_counts()\n",
    "        self.logger.info(f\"Class distribution: {dict(class_counts)}\")\n",
    "\n",
    "        total_classes = len(class_counts)\n",
    "        base_samples_per_class = self.config.sample_size // total_classes\n",
    "\n",
    "        sampled_dfs = []\n",
    "        for class_name in class_counts.index:\n",
    "            class_df = df[df['class'] == class_name]\n",
    "\n",
    "            samples_needed = min(len(class_df), max(base_samples_per_class, 100))\n",
    "\n",
    "            if len(class_df) > samples_needed:\n",
    "                class_sample = class_df.sample(n=samples_needed, random_state=42)\n",
    "            else:\n",
    "                class_sample = class_df\n",
    "\n",
    "            sampled_dfs.append(class_sample)\n",
    "            self.logger.info(f\"Class '{class_name}': {len(class_sample)} samples\")\n",
    "\n",
    "        return pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "    def _stratify_by_length(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Stratify sampling by text length to ensure diverse examples.\"\"\"\n",
    "        df['text_length'] = df['before'].str.len()\n",
    "\n",
    "        df['length_category'] = pd.cut(\n",
    "            df['text_length'],\n",
    "            bins=[0, 20, 50, 100, float('inf')],\n",
    "            labels=['short', 'medium', 'long', 'very_long']\n",
    "        )\n",
    "\n",
    "        length_counts = df['length_category'].value_counts()\n",
    "        self.logger.info(f\"Length distribution: {dict(length_counts)}\")\n",
    "\n",
    "        samples_per_length = self.config.sample_size\n",
    "\n",
    "        sampled_dfs = []\n",
    "        for length_cat in ['short', 'medium', 'long', 'very_long']:\n",
    "            cat_df = df[df['length_category'] == length_cat]\n",
    "\n",
    "            if len(cat_df) == 0:\n",
    "                continue\n",
    "\n",
    "            samples_needed = min(len(cat_df), samples_per_length)\n",
    "\n",
    "            if len(cat_df) > samples_needed:\n",
    "                cat_sample = cat_df.sample(n=samples_needed, random_state=42)\n",
    "            else:\n",
    "                cat_sample = cat_df\n",
    "\n",
    "            sampled_dfs.append(cat_sample)\n",
    "            self.logger.info(f\"Length '{length_cat}': {len(cat_sample)} samples\")\n",
    "\n",
    "        result_df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "\n",
    "        return result_df.drop(['text_length', 'length_category'], axis=1)\n",
    "\n",
    "class FastTextNormalizationDataset(Dataset):\n",
    "    \"\"\"Optimized dataset class for faster training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: List[Tuple[str, str]],\n",
    "        tokenizer: T5Tokenizer,\n",
    "        max_source_length: int = 96,\n",
    "        max_target_length: int = 96\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "        logger.info(f\"Initialized fast dataset with {len(self.data)} examples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_text, target_text = self.data[idx]\n",
    "\n",
    "        source_text = f\"normalize: {source_text}\"\n",
    "\n",
    "        source_encoding = self.tokenizer(\n",
    "            source_text,\n",
    "            max_length=self.max_source_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': source_encoding['input_ids'].flatten(),\n",
    "            'attention_mask': source_encoding['attention_mask'].flatten(),\n",
    "            'labels': target_encoding['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "class FastT5TextNormalizer:\n",
    "    \"\"\"Optimized T5 text normalizer for fast training.\"\"\"\n",
    "\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "            print(\"Using Apple Silicon MPS backend\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "            print(\"Using CUDA\")\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            print(\"Using CPU\")\n",
    "\n",
    "        logger.info(f\"Initializing Fast T5 model on device: {self.device}\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(config.model_name)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.use_amp = config.mixed_precision and (self.device.type in ['cuda', 'mps'])\n",
    "        if self.use_amp:\n",
    "            self.scaler = GradScaler()\n",
    "            print(\"Using Automatic Mixed Precision\")\n",
    "\n",
    "        if config.compile_model and hasattr(torch, 'compile'):\n",
    "            self.model = torch.compile(self.model)\n",
    "            print(\"Model compiled with PyTorch 2.0\")\n",
    "        if hasattr(self.model, 'gradient_checkpointing_enable'):\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        if hasattr(self.model.config, 'use_cache'):\n",
    "            self.model.config.use_cache = False\n",
    "        if self.device.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "\n",
    "        logger.info(f\"Loaded model: {config.model_name}\")\n",
    "\n",
    "    def train_fast(self, train_data: List[Tuple[str, str]], val_data: List[Tuple[str, str]]):\n",
    "        \"\"\"Fast training method (as I myself cannot do long trainings :[ ).\"\"\"\n",
    "        logger.info(f\"Starting FAST T5 training with {len(train_data)} train, {len(val_data)} val examples\")\n",
    "\n",
    "        if self.config.use_wandb:\n",
    "            wandb.init(\n",
    "                project=\"russian-text-normalization-fast\",\n",
    "                config=self.config.__dict__,\n",
    "                name=f\"ruT5-fast-{self.config.sample_size}\"\n",
    "            )\n",
    "\n",
    "        train_dataset = FastTextNormalizationDataset(\n",
    "            train_data, self.tokenizer,\n",
    "            self.config.max_source_length, self.config.max_target_length\n",
    "        )\n",
    "        val_dataset = FastTextNormalizationDataset(\n",
    "            val_data, self.tokenizer,\n",
    "            self.config.max_source_length, self.config.max_target_length\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.config.dataloader_num_workers,\n",
    "            pin_memory=self.config.pin_memory,\n",
    "            drop_last=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            eps=1e-6\n",
    "        )\n",
    "\n",
    "        total_steps = len(train_loader) * self.config.num_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.model.train()\n",
    "        global_step = 0\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            logger.info(f\"Starting epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "\n",
    "            epoch_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                batch = {k: v.to(self.device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "                if self.use_amp:\n",
    "                    with autocast():\n",
    "                        outputs = self.model(**batch)\n",
    "                        loss = outputs.loss\n",
    "\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(),\n",
    "                        self.config.gradient_clip_val\n",
    "                    )\n",
    "                    self.scaler.step(optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    outputs = self.model(**batch)\n",
    "                    loss = outputs.loss\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(),\n",
    "                        self.config.gradient_clip_val\n",
    "                    )\n",
    "                    optimizer.step()\n",
    "\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                global_step += 1\n",
    "\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f\"{loss.item():.4f}\",\n",
    "                    'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "                })\n",
    "\n",
    "                if self.config.use_wandb and global_step % 50 == 0:\n",
    "                    wandb.log({\n",
    "                        \"train_loss\": loss.item(),\n",
    "                        \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "                        \"epoch\": epoch,\n",
    "                        \"global_step\": global_step\n",
    "                    })\n",
    "\n",
    "                if global_step % self.config.eval_steps == 0:\n",
    "                    val_loss = self._quick_validate(val_loader)\n",
    "                    logger.info(f\"Step {global_step}: Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "                    if self.config.use_wandb:\n",
    "                        wandb.log({\"val_loss\": val_loss, \"global_step\": global_step})\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        self._save_model(\"best_model_fast\")\n",
    "                        logger.info(f\"New best model saved! Val loss: {val_loss:.4f}\")\n",
    "\n",
    "                    self.model.train()\n",
    "                if self.device.type == 'mps' and batch_idx % 100 == 0:\n",
    "                    torch.mps.empty_cache()\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "            logger.info(f\"Epoch {epoch + 1} completed. Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "        self._save_model(\"final_model_fast\")\n",
    "        logger.info(\"Fast training completed!\")\n",
    "\n",
    "        if self.config.use_wandb:\n",
    "            wandb.finish()\n",
    "\n",
    "    def _quick_validate(self, val_loader: DataLoader) -> float:\n",
    "        \"\"\"Quick validation on subset of validation data.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        max_val_batches = 10\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                if batch_idx >= max_val_batches:\n",
    "                    break\n",
    "\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                total_loss += outputs.loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "        return total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "\n",
    "    def _save_model(self, name: str):\n",
    "        \"\"\"Save model and tokenizer.\"\"\"\n",
    "        save_path = Path(f\"./models/{name}\")\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.model.save_pretrained(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "\n",
    "        with open(save_path / \"config.json\", \"w\") as f:\n",
    "            json.dump(self.config.__dict__, f, indent=2)\n",
    "\n",
    "        logger.info(f\"Model saved to {save_path}\")\n",
    "\n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Normalize text using trained T5 model.\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        try:\n",
    "            input_text = f\"normalize: {text}\"\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                max_length=self.config.max_source_length,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=self.config.max_target_length,\n",
    "                    num_beams=2,\n",
    "                    length_penalty=0.6,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False\n",
    "                )\n",
    "\n",
    "            # Decode\n",
    "            normalized_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return normalized_text.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error normalizing text: {str(e)}\")\n",
    "            return text\n",
    "\n",
    "def fast_training_pipeline(data_path: str):\n",
    "    \"\"\"Optimized training pipeline for quick experimentation.\"\"\"\n",
    "    logger.info(\"Starting FAST T5 Russian Text Normalization Training\")\n",
    "\n",
    "    config = TrainingConfig(\n",
    "        model_name=\"ai-forever/ruT5-base\",\n",
    "        batch_size=32,\n",
    "        learning_rate=5e-4,\n",
    "        num_epochs=3,\n",
    "        sample_size=50000,\n",
    "        use_wandb=True,\n",
    "        max_source_length=96,\n",
    "        max_target_length=96\n",
    "    )\n",
    "\n",
    "\n",
    "    cleaner = RobustRussianTextCleaner()\n",
    "    sampler = StratifiedDataSampler(config)\n",
    "\n",
    "\n",
    "    logger.info(\"Loading and cleaning data...\")\n",
    "    df = cleaner.safe_load_csv(data_path)\n",
    "    df_clean = cleaner.clean_dataset(df, aggressive_cleaning=False)  # Gentle cleaning\n",
    "\n",
    "\n",
    "    df_sample = sampler.sample_dataset(df_clean)\n",
    "\n",
    "\n",
    "    if 'before' in df_sample.columns and 'after' in df_sample.columns:\n",
    "        data_pairs = list(zip(df_sample['before'].astype(str), df_sample['after'].astype(str)))\n",
    "    else:\n",
    "        raise ValueError(\"Expected 'before' and 'after' columns\")\n",
    "\n",
    "    train_data, val_data = train_test_split(data_pairs, test_size=0.1, random_state=42)\n",
    "\n",
    "    logger.info(f\"Training on {len(train_data)} examples, validating on {len(val_data)}\")\n",
    "\n",
    "    model = FastT5TextNormalizer(config)\n",
    "    model.train_fast(train_data, val_data)\n",
    "\n",
    "    logger.info(\"Running quick evaluation...\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (source, target) in enumerate(val_data[:100]):\n",
    "        pred = model.normalize_text(source)\n",
    "        if pred.strip().lower() == target.strip().lower():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "        if i < 5:\n",
    "            logger.info(f\"Example {i+1}:\")\n",
    "            logger.info(f\"  Input:  {source}\")\n",
    "            logger.info(f\"  Target: {target}\")\n",
    "            logger.info(f\"  Pred:   {pred}\")\n",
    "            logger.info(f\"  Match:  {pred.strip().lower() == target.strip().lower()}\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    logger.info(f\"Quick accuracy on 100 examples: {accuracy:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n"
   ],
   "id": "680e443145780486",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaskazutin/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-31T15:11:13.010146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "if __name__ == '__main__':\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('fork', force=True)\n",
    "\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "    torch.set_num_threads(8)\n",
    "\n",
    "    DATA_PATH = 'data/to_normalize.csv'\n",
    "    OUT_PATH = 'data/normalized_llm.csv'\n",
    "    model = fast_training_pipeline(\"data/ru_train.csv\")"
   ],
   "id": "f1b1d8045427da33",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 22:11:13,014 - __main__ - INFO - Starting FAST T5 Russian Text Normalization Training\n",
      "2025-05-31 22:11:13,017 - __main__ - INFO - Loading and cleaning data...\n",
      "2025-05-31 22:11:13,018 - __main__ - INFO - Trying to load with encoding: utf-8\n",
      "2025-05-31 22:11:18,533 - __main__ - INFO - Successfully loaded with utf-8\n",
      "2025-05-31 22:11:18,534 - __main__ - INFO - Starting robust cleaning of 10574516 rows\n",
      "2025-05-31 22:11:22,155 - __main__ - INFO - Removed 14 clearly empty rows\n",
      "2025-05-31 22:11:22,156 - __main__ - INFO - Basic data handling: 10574516 -> 10574502 (14 removed, 0.0%)\n",
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_57585/4107402627.py:195: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['before'] = df['before'].apply(fix_encoding)\n",
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_57585/4107402627.py:196: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['after'] = df['after'].apply(fix_encoding)\n",
      "2025-05-31 22:11:36,285 - __main__ - INFO - Character encoding fix: 10574516 -> 10574502 (14 removed, 0.0%)\n",
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_57585/4107402627.py:224: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['before'] = df['before'].apply(lambda x: clean_text(x, gentle))\n",
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_57585/4107402627.py:225: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['after'] = df['after'].apply(lambda x: clean_text(x, gentle))\n",
      "2025-05-31 22:12:09,738 - __main__ - INFO - Character cleaning removed 0 rows\n",
      "2025-05-31 22:12:09,765 - __main__ - INFO - Problematic character removal: 10574516 -> 10574502 (14 removed, 0.0%)\n",
      "2025-05-31 22:12:12,323 - __main__ - INFO - Duplicate removal: 9758444 exact duplicates removed\n",
      "2025-05-31 22:12:12,423 - __main__ - INFO - Clear duplicate handling: 10574516 -> 816058 (9758458 removed, 92.3%)\n",
      "2025-05-31 22:12:12,714 - __main__ - INFO - Final cleanup removed 4 overly long texts\n",
      "2025-05-31 22:12:12,715 - __main__ - INFO - Minimal final cleanup: 10574516 -> 816054 (9758462 removed, 92.3%)\n",
      "2025-05-31 22:12:12,716 - __main__ - INFO - Gentle cleaning complete: 10574516 -> 816054 rows (92.3% removed)\n",
      "2025-05-31 22:12:12,745 - __main__ - INFO - Creating stratified sample of 50000 from 816054 rows\n",
      "2025-05-31 22:12:12,766 - __main__ - INFO - Class distribution: {'PLAIN': np.int64(696259), 'DATE': np.int64(41733), 'MEASURE': np.int64(17553), 'LETTERS': np.int64(16139), 'CARDINAL': np.int64(15599), 'TELEPHONE': np.int64(8496), 'ORDINAL': np.int64(5237), 'DECIMAL': np.int64(4270), 'ELECTRONIC': np.int64(3706), 'VERBATIM': np.int64(2479), 'MONEY': np.int64(2395), 'FRACTION': np.int64(1101), 'TIME': np.int64(771), 'DIGIT': np.int64(294), 'PUNCT': np.int64(22)}\n",
      "2025-05-31 22:12:12,815 - __main__ - INFO - Class 'PLAIN': 3333 samples\n",
      "2025-05-31 22:12:12,847 - __main__ - INFO - Class 'DATE': 3333 samples\n",
      "2025-05-31 22:12:12,870 - __main__ - INFO - Class 'MEASURE': 3333 samples\n",
      "2025-05-31 22:12:12,890 - __main__ - INFO - Class 'LETTERS': 3333 samples\n",
      "2025-05-31 22:12:12,911 - __main__ - INFO - Class 'CARDINAL': 3333 samples\n",
      "2025-05-31 22:12:12,932 - __main__ - INFO - Class 'TELEPHONE': 3333 samples\n",
      "2025-05-31 22:12:12,952 - __main__ - INFO - Class 'ORDINAL': 3333 samples\n",
      "2025-05-31 22:12:12,972 - __main__ - INFO - Class 'DECIMAL': 3333 samples\n",
      "2025-05-31 22:12:12,992 - __main__ - INFO - Class 'ELECTRONIC': 3333 samples\n",
      "2025-05-31 22:12:13,011 - __main__ - INFO - Class 'VERBATIM': 2479 samples\n",
      "2025-05-31 22:12:13,032 - __main__ - INFO - Class 'MONEY': 2395 samples\n",
      "2025-05-31 22:12:13,052 - __main__ - INFO - Class 'FRACTION': 1101 samples\n",
      "2025-05-31 22:12:13,073 - __main__ - INFO - Class 'TIME': 771 samples\n",
      "2025-05-31 22:12:13,094 - __main__ - INFO - Class 'DIGIT': 294 samples\n",
      "2025-05-31 22:12:13,115 - __main__ - INFO - Class 'PUNCT': 22 samples\n",
      "2025-05-31 22:12:13,126 - __main__ - INFO - Length distribution: {'short': np.int64(36121), 'medium': np.int64(777), 'long': np.int64(131), 'very_long': np.int64(30)}\n",
      "2025-05-31 22:12:13,128 - __main__ - INFO - Length 'short': 12500 samples\n",
      "2025-05-31 22:12:13,130 - __main__ - INFO - Length 'medium': 777 samples\n",
      "2025-05-31 22:12:13,131 - __main__ - INFO - Length 'long': 131 samples\n",
      "2025-05-31 22:12:13,132 - __main__ - INFO - Length 'very_long': 30 samples\n",
      "2025-05-31 22:12:13,134 - __main__ - INFO - Final sample size: 13438\n",
      "2025-05-31 22:12:13,448 - __main__ - INFO - Training on 12094 examples, validating on 1344\n",
      "2025-05-31 22:12:13,448 - __main__ - INFO - Initializing Fast T5 model on device: mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon MPS backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_57585/4107402627.py:460: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n",
      "/Users/nikitaskazutin/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "2025-05-31 22:12:22,705 - __main__ - INFO - Loaded model: ai-forever/ruT5-base\n",
      "2025-05-31 22:12:22,705 - __main__ - INFO - Starting FAST T5 training with 12094 train, 1344 val examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Automatic Mixed Precision\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruT5-fast-50000</strong> at: <a href='https://wandb.ai/skazhutin-n-hits/russian-text-normalization-fast/runs/dfhhbew2' target=\"_blank\">https://wandb.ai/skazhutin-n-hits/russian-text-normalization-fast/runs/dfhhbew2</a><br> View project at: <a href='https://wandb.ai/skazhutin-n-hits/russian-text-normalization-fast' target=\"_blank\">https://wandb.ai/skazhutin-n-hits/russian-text-normalization-fast</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250531_220701-dfhhbew2/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "creating run (0.0s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/nikitaskazutin/PycharmProjects/PythonProject/wandb/run-20250531_221222-2xpbrkfb</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skazhutin-n-hits/russian-text-normalization-fast/runs/2xpbrkfb' target=\"_blank\">ruT5-fast-50000</a></strong> to <a href='https://wandb.ai/skazhutin-n-hits/russian-text-normalization-fast' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skazhutin-n-hits/russian-text-normalization-fast' target=\"_blank\">https://wandb.ai/skazhutin-n-hits/russian-text-normalization-fast</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skazhutin-n-hits/russian-text-normalization-fast/runs/2xpbrkfb' target=\"_blank\">https://wandb.ai/skazhutin-n-hits/russian-text-normalization-fast/runs/2xpbrkfb</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 22:13:39,764 - __main__ - INFO - Initialized fast dataset with 12094 examples\n",
      "2025-05-31 22:13:39,765 - __main__ - INFO - Initialized fast dataset with 1344 examples\n",
      "2025-05-31 22:13:39,770 - __main__ - INFO - Starting epoch 1/3\n",
      "Epoch 1:   0%|          | 0/377 [00:00<?, ?it/s]/Users/nikitaskazutin/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/var/folders/4k/r28hjx957zd3v1xzgv0q81lr0000gn/T/ipykernel_57585/4107402627.py:549: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/Users/nikitaskazutin/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Epoch 1:  66%|██████▌   | 249/377 [10:33<05:13,  2.45s/it, loss=0.3924, lr=4.17e-04]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:20:28.226873Z",
     "start_time": "2025-05-31T14:20:28.221918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import psutil\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.step_times = []\n",
    "\n",
    "    def start_epoch(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def log_step(self, step, loss):\n",
    "        current_time = time.time()\n",
    "        if self.start_time:\n",
    "            step_time = current_time - self.start_time\n",
    "            self.step_times.append(step_time)\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                avg_step_time = sum(self.step_times[-50:]) / len(self.step_times[-50:])\n",
    "                memory_usage = psutil.virtual_memory().percent\n",
    "                print(f\"Step {step}: Loss={loss:.4f}, Avg Step Time={avg_step_time:.2f}s, Memory={memory_usage:.1f}%\")\n",
    "\n",
    "        self.start_time = current_time"
   ],
   "id": "70d57e7f5c62ced3",
   "outputs": [],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
