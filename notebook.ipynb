{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T06:59:57.078469Z",
     "start_time": "2025-06-01T06:59:56.942630Z"
    }
   },
   "source": [
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Advanced imports for T5 training\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('./data/log_file.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Enhanced configuration for text normalization training.\"\"\"\n",
    "    model_name: str = \"ai-forever/ruT5-base\"\n",
    "    max_source_length: int = 128\n",
    "    max_target_length: int = 128\n",
    "\n",
    "    batch_size: int = 8\n",
    "    learning_rate: float = 1e-4\n",
    "    num_epochs: int = 1\n",
    "    warmup_steps: int = 100\n",
    "    weight_decay: float = 0.05\n",
    "    gradient_clip_val: float = 0.5\n",
    "\n",
    "    sample_size: int = 15000\n",
    "    stratify_by_class: bool = True\n",
    "    stratify_by_length: bool = True\n",
    "\n",
    "    dataloader_num_workers: int = 4\n",
    "    mixed_precision: bool = True\n",
    "    compile_model: bool = False\n",
    "    pin_memory: bool = True\n",
    "    seed: int = 42\n",
    "\n",
    "class RobustRussianTextCleaner:\n",
    "    \"\"\"Advanced text cleaner with robust character handling.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.russian_letters = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ')\n",
    "        self.valid_chars = (\n",
    "            self.russian_letters |\n",
    "            set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789') |\n",
    "            set(' .,!?-()[]{}\\\";:\\'\\\"/@#$%^&*+=<>~`|\\\\€$₽°')\n",
    "        )\n",
    "\n",
    "    def safe_load_csv(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Safely load CSV with encoding issues handling.\"\"\"\n",
    "        encodings_to_try = ['utf-8', 'utf-8-sig', 'cp1251', 'iso-8859-1', 'latin1']\n",
    "\n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                self.logger.info(f\"Trying to load with encoding: {encoding}\")\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "                self.logger.info(f\"Successfully loaded with {encoding}\")\n",
    "                return df\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Failed with {encoding}: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            self.logger.info(\"Loading with error handling (replacing bad characters)\")\n",
    "            df = pd.read_csv(file_path, encoding='utf-8', errors='replace')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"All encoding attempts failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def clean_dataset(self, df: pd.DataFrame, aggressive_cleaning: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Clean Russian text normalization dataset with robust character handling.\"\"\"\n",
    "        self.logger.info(f\"Starting robust cleaning of {len(df)} rows\")\n",
    "        original_size = len(df)\n",
    "\n",
    "        df = self._handle_basic_issues(df)\n",
    "        df = self._fix_character_encoding(df)\n",
    "        df = self._clean_problematic_characters(df, gentle=True)\n",
    "\n",
    "        if aggressive_cleaning:\n",
    "            df = self._validate_normalizations(df)\n",
    "\n",
    "        df = self._handle_clear_duplicates(df)\n",
    "        df = self._minimal_final_cleanup(df)\n",
    "\n",
    "        cleaned_size = len(df)\n",
    "        removal_percentage = ((original_size - cleaned_size) / original_size) * 100\n",
    "        self.logger.info(f\"Cleaning complete: {original_size} -> {cleaned_size} rows \"\n",
    "                        f\"({removal_percentage:.1f}% removed)\")\n",
    "\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def _handle_basic_issues(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handle basic data integrity issues.\"\"\"\n",
    "        required_cols = ['before', 'after']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "        df['before'] = df['before'].astype(str).replace('nan', '')\n",
    "        df['after'] = df['after'].astype(str).replace('nan', '')\n",
    "\n",
    "        initial_len = len(df)\n",
    "        df = df[(df['before'].str.strip() != '') & (df['after'].str.strip() != '')]\n",
    "        self.logger.info(f\"Removed {initial_len - len(df)} clearly empty rows\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _fix_character_encoding(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Fix character encoding issues.\"\"\"\n",
    "        def fix_encoding(text):\n",
    "            if pd.isna(text):\n",
    "                return \"\"\n",
    "\n",
    "            text = str(text)\n",
    "\n",
    "            try:\n",
    "                text = unicodedata.normalize('NFKC', text)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            encoding_fixes = {\n",
    "                'Ã¡': 'а', 'Ã ': 'а', 'Ã«': 'е', 'Ã¬': 'и', 'Ã®': 'о', 'Ã³': 'у',\n",
    "                'â€œ': '\"', 'â€': '\"', 'â€™': \"'\", 'â€\"': '–', 'â€\"': '—'\n",
    "            }\n",
    "\n",
    "            for wrong, correct in encoding_fixes.items():\n",
    "                text = text.replace(wrong, correct)\n",
    "\n",
    "            return text\n",
    "\n",
    "        df['before'] = df['before'].apply(fix_encoding)\n",
    "        df['after'] = df['after'].apply(fix_encoding)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _clean_problematic_characters(self, df: pd.DataFrame, gentle: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Remove problematic characters with gentle approach.\"\"\"\n",
    "        def clean_text(text, gentle_mode=True):\n",
    "            if pd.isna(text):\n",
    "                return \"\"\n",
    "\n",
    "            text = str(text)\n",
    "\n",
    "            if gentle_mode:\n",
    "                problematic_chars = set(['', '', '', '﻿', '\\x00', '\\x01', '\\x02', '\\x03'])\n",
    "                text = ''.join(char for char in text if char not in problematic_chars)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            else:\n",
    "                text = ''.join(char for char in text if char in self.valid_chars or char.isspace())\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            return text\n",
    "\n",
    "        initial_len = len(df)\n",
    "\n",
    "        df['before'] = df['before'].apply(lambda x: clean_text(x, gentle))\n",
    "        df['after'] = df['after'].apply(lambda x: clean_text(x, gentle))\n",
    "\n",
    "        df = df[(df['before'].str.strip() != '') & (df['after'].str.strip() != '')]\n",
    "\n",
    "        self.logger.info(f\"Character cleaning removed {initial_len - len(df)} rows\")\n",
    "        return df\n",
    "\n",
    "    def _validate_normalizations(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Validate normalizations (only if aggressive cleaning is enabled).\"\"\"\n",
    "        initial_len = len(df)\n",
    "\n",
    "        def is_valid_normalization(before, after):\n",
    "            if len(after) > len(before) * 5:\n",
    "                return False\n",
    "            if len(after) < len(before) * 0.1 and len(before) > 10:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        mask = df.apply(lambda row: is_valid_normalization(row['before'], row['after']), axis=1)\n",
    "        df = df[mask]\n",
    "\n",
    "        self.logger.info(f\"Normalization validation removed {initial_len - len(df)} rows\")\n",
    "        return df\n",
    "\n",
    "    def _handle_clear_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Handle only clear duplicates.\"\"\"\n",
    "        initial_len = len(df)\n",
    "        df = df.drop_duplicates(subset=['before', 'after'])\n",
    "        self.logger.info(f\"Duplicate removal: {initial_len - len(df)} exact duplicates removed\")\n",
    "        return df\n",
    "\n",
    "    def _minimal_final_cleanup(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Minimal final cleanup.\"\"\"\n",
    "        initial_len = len(df)\n",
    "        df = df[df['before'].str.len() <= 500]\n",
    "        df = df[df['after'].str.len() <= 600]\n",
    "        self.logger.info(f\"Final cleanup removed {initial_len - len(df)} overly long texts\")\n",
    "        return df\n",
    "\n",
    "class AdvancedRussianTextNormalizer:\n",
    "    \"\"\"\n",
    "    Enhanced Russian text normalization model combining rule-based and neural approaches.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the enhanced text normalization model.\"\"\"\n",
    "        logger.info(\"Initializing Advanced Russian Text Normalization Model\")\n",
    "\n",
    "        # File paths\n",
    "        self.dictionary_path = 'data/dictionary/model_dictionary.json'\n",
    "        self.results_path = 'data/results.csv'\n",
    "        self.train_path = 'data/inputs/ru_train.csv'\n",
    "        self.test_path = 'data/inputs/ru_test_2.csv'\n",
    "        self.result_path = 'data/results/result.csv'\n",
    "\n",
    "        # Create necessary directories\n",
    "        os.makedirs('data/dictionary', exist_ok=True)\n",
    "        os.makedirs('data/results', exist_ok=True)\n",
    "        os.makedirs('models_cache', exist_ok=True)\n",
    "\n",
    "        # Initialize text cleaner\n",
    "        self.cleaner = RobustRussianTextCleaner()\n",
    "\n",
    "        # Enhanced dictionaries for rule-based normalization\n",
    "        self.numbers = {\n",
    "            '0': 'ноль', '1': 'один', '2': 'два', '3': 'три', '4': 'четыре',\n",
    "            '5': 'пять', '6': 'шесть', '7': 'семь', '8': 'восемь', '9': 'девять',\n",
    "            '10': 'десять', '11': 'одиннадцать', '12': 'двенадцать',\n",
    "            '13': 'тринадцать', '14': 'четырнадцать', '15': 'пятнадцать',\n",
    "            '16': 'шестнадцать', '17': 'семнадцать', '18': 'восемнадцать',\n",
    "            '19': 'девятнадцать', '20': 'двадцать', '30': 'тридцать',\n",
    "            '40': 'сорок', '50': 'пятьдесят', '60': 'шестьдесят',\n",
    "            '70': 'семьдесят', '80': 'восемьдесят', '90': 'девяносто'\n",
    "        }\n",
    "\n",
    "        self.hundreds = {\n",
    "            '100': 'сто', '200': 'двести', '300': 'триста', '400': 'четыреста',\n",
    "            '500': 'пятьсот', '600': 'шестьсот', '700': 'семьсот',\n",
    "            '800': 'восемьсот', '900': 'девятьсот'\n",
    "        }\n",
    "\n",
    "        self.ordinals = {\n",
    "            '1': 'первое', '2': 'второе', '3': 'третье', '4': 'четвертое',\n",
    "            '5': 'пятое', '6': 'шестое', '7': 'седьмое', '8': 'восьмое',\n",
    "            '9': 'девятое', '10': 'десятое', '11': 'одиннадцатое',\n",
    "            '12': 'двенадцатое', '13': 'тринадцатое', '14': 'четырнадцатое',\n",
    "            '15': 'пятнадцатое', '16': 'шестнадцатое', '17': 'семнадцатое',\n",
    "            '18': 'восемнадцатое', '19': 'девятнадцатое', '20': 'двадцатое',\n",
    "            '21': 'двадцать первое', '22': 'двадцать второе', '23': 'двадцать третье',\n",
    "            '24': 'двадцать четвертое', '25': 'двадцать пятое', '26': 'двадцать шестое',\n",
    "            '27': 'двадцать седьмое', '28': 'двадцать восьмое', '29': 'двадцать девятое',\n",
    "            '30': 'тридцатое', '31': 'тридцать первое'\n",
    "        }\n",
    "\n",
    "        self.months = {\n",
    "            '1': 'января', '2': 'февраля', '3': 'марта', '4': 'апреля',\n",
    "            '5': 'мая', '6': 'июня', '7': 'июля', '8': 'августа',\n",
    "            '9': 'сентября', '10': 'октября', '11': 'ноября', '12': 'декабря',\n",
    "            '01': 'января', '02': 'февраля', '03': 'марта', '04': 'апреля',\n",
    "            '05': 'мая', '06': 'июня', '07': 'июля', '08': 'августа',\n",
    "            '09': 'сентября'\n",
    "        }\n",
    "\n",
    "        self.roman_numerals = {\n",
    "            'I': 'первый', 'II': 'второй', 'III': 'третий', 'IV': 'четвертый',\n",
    "            'V': 'пятый', 'VI': 'шестой', 'VII': 'седьмой', 'VIII': 'восьмой',\n",
    "            'IX': 'девятый', 'X': 'десятый', 'XI': 'одиннадцатый',\n",
    "            'XII': 'двенадцатый', 'XIII': 'тринадцатый', 'XIV': 'четырнадцатый',\n",
    "            'XV': 'пятнадцатый', 'XVI': 'шестнадцатый', 'XVII': 'семнадцатый',\n",
    "            'XVIII': 'восемнадцатый', 'XIX': 'девятнадцатый', 'XX': 'двадцатый'\n",
    "        }\n",
    "\n",
    "        logger.info(\"Advanced Russian Text Normalization Model initialized successfully\")\n",
    "\n",
    "    def train_enhanced_dict(self):\n",
    "        \"\"\"Create enhanced dictionary with robust data cleaning.\"\"\"\n",
    "        logger.info(\"Starting enhanced dictionary creation...\")\n",
    "\n",
    "        # Load and clean training data\n",
    "        train_df = self.cleaner.safe_load_csv(self.train_path)\n",
    "        if train_df is None:\n",
    "            logger.error(\"Failed to load training data\")\n",
    "            return\n",
    "\n",
    "        # Clean the dataset\n",
    "        train_df = self.cleaner.clean_dataset(train_df, aggressive_cleaning=False)\n",
    "\n",
    "        # Process the data\n",
    "        train_df['before'] = train_df['before'].str.lower()\n",
    "        train_df['after'] = train_df['after'].str.lower()\n",
    "        train_df['after_c'] = train_df['after'].map(lambda x: len(str(x).split()))\n",
    "        train_df = train_df[~((train_df['class'] == 'LETTERS') & (train_df['after_c'] > 4))]\n",
    "\n",
    "        # Group and get most frequent normalizations\n",
    "        train_df = train_df.groupby(['before', 'after'], as_index=False)['sentence_id'].count()\n",
    "        train_df = train_df.sort_values(['sentence_id', 'before'], ascending=[False, True])\n",
    "        train_df = train_df.drop_duplicates(['before'])\n",
    "\n",
    "        # Create dictionary\n",
    "        dictionary = {key: value for (key, value) in train_df[['before', 'after']].values}\n",
    "        logger.info(f\"Dictionary created with {len(dictionary)} entries\")\n",
    "\n",
    "        # Save dictionary\n",
    "        try:\n",
    "            with open(self.dictionary_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(dictionary, f, indent=4, ensure_ascii=False)\n",
    "            logger.info(f\"Dictionary saved to {self.dictionary_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save dictionary: {e}\")\n",
    "\n",
    "        logger.info(\"Enhanced dictionary creation completed successfully\")\n",
    "\n",
    "    def normalize_with_rules(self, text: str) -> str:\n",
    "        \"\"\"Apply rule-based normalization to text.\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        try:\n",
    "            normalized_text = self._preprocess_text(text)\n",
    "            normalized_text = self._normalize_dates(normalized_text)\n",
    "            normalized_text = self._normalize_time(normalized_text)\n",
    "            normalized_text = self._normalize_currency(normalized_text)\n",
    "            normalized_text = self._normalize_measurements(normalized_text)\n",
    "            normalized_text = self._normalize_percentages(normalized_text)\n",
    "            normalized_text = self._normalize_phone_numbers(normalized_text)\n",
    "            normalized_text = self._normalize_urls_emails(normalized_text)\n",
    "            normalized_text = self._normalize_abbreviations(normalized_text)\n",
    "            normalized_text = self._normalize_roman_numerals(normalized_text)\n",
    "            normalized_text = self._normalize_numbers(normalized_text)\n",
    "            normalized_text = self._normalize_punctuation(normalized_text)\n",
    "            normalized_text = self._postprocess_text(normalized_text)\n",
    "\n",
    "            return normalized_text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during rule-based normalization: {str(e)}\")\n",
    "            return text\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess text by cleaning and standardizing format.\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        text = text.replace('—', '-').replace('–', '-')\n",
    "        text = text.replace('\"', '\"').replace('\"', '\"')\n",
    "        text = text.replace(''', \"'\").replace(''', \"'\")\n",
    "        return text\n",
    "\n",
    "    def _normalize_dates(self, text: str) -> str:\n",
    "        \"\"\"Normalize dates in various formats.\"\"\"\n",
    "        def convert_date(match):\n",
    "            day, month, year = match.groups()\n",
    "            day_word = self.ordinals.get(str(int(day)), f\"{day}-е\")\n",
    "            month_word = self.months.get(str(int(month)), month)\n",
    "            year_word = self._convert_year(year)\n",
    "            return f\"{day_word} {month_word} {year_word} года\"\n",
    "\n",
    "        text = re.sub(r'\\b(\\d{1,2})[./](\\d{1,2})[./](\\d{4})\\b', convert_date, text)\n",
    "\n",
    "        def convert_short_date(match):\n",
    "            day, month = match.groups()\n",
    "            day_word = self.ordinals.get(str(int(day)), f\"{day}-е\")\n",
    "            month_word = self.months.get(str(int(month)), month)\n",
    "            return f\"{day_word} {month_word}\"\n",
    "\n",
    "        text = re.sub(r'\\b(\\d{1,2})[./](\\d{1,2})\\b(?!/)', convert_short_date, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_time(self, text: str) -> str:\n",
    "        \"\"\"Normalize time expressions.\"\"\"\n",
    "        def convert_time(match):\n",
    "            hours, minutes = match.groups()\n",
    "            hour_int = int(hours)\n",
    "            minute_int = int(minutes)\n",
    "\n",
    "            hour_word = self._number_to_words(hour_int)\n",
    "\n",
    "            if minute_int == 0:\n",
    "                return f\"{hour_word} часов\"\n",
    "            else:\n",
    "                minute_word = self._number_to_words(minute_int)\n",
    "                return f\"{hour_word} часов {minute_word} минут\"\n",
    "\n",
    "        text = re.sub(r'\\b(\\d{1,2}):(\\d{2})\\b', convert_time, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_currency(self, text: str) -> str:\n",
    "        \"\"\"Normalize currency expressions.\"\"\"\n",
    "        def convert_currency(match):\n",
    "            amount, currency = match.groups()\n",
    "            amount_int = int(amount)\n",
    "            amount_word = self._number_to_words(amount_int)\n",
    "\n",
    "            if currency == '₽' or currency == 'руб':\n",
    "                if amount_int == 1:\n",
    "                    currency_word = 'рубль'\n",
    "                elif 2 <= amount_int <= 4:\n",
    "                    currency_word = 'рубля'\n",
    "                else:\n",
    "                    currency_word = 'рублей'\n",
    "            elif currency == '$':\n",
    "                if amount_int == 1:\n",
    "                    currency_word = 'доллар'\n",
    "                elif 2 <= amount_int <= 4:\n",
    "                    currency_word = 'доллара'\n",
    "                else:\n",
    "                    currency_word = 'долларов'\n",
    "            elif currency == '€':\n",
    "                currency_word = 'евро'\n",
    "            else:\n",
    "                currency_word = currency\n",
    "\n",
    "            return f\"{amount_word} {currency_word}\"\n",
    "\n",
    "        text = re.sub(r'(\\d+)\\s*([₽$€]|руб\\.?)', convert_currency, text)\n",
    "\n",
    "        def convert_decimal_currency(match):\n",
    "            amount, currency = match.groups()\n",
    "            amount_word = self._convert_decimal_number(amount)\n",
    "            currency_word = self._get_currency_word(currency)\n",
    "            return f\"{amount_word} {currency_word}\"\n",
    "\n",
    "        text = re.sub(r'(\\d+[.,]\\d+)\\s*([₽$€]|руб\\.?)', convert_decimal_currency, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_measurements(self, text: str) -> str:\n",
    "        \"\"\"Normalize measurement units.\"\"\"\n",
    "        def convert_measurement(match):\n",
    "            amount, unit = match.groups()\n",
    "\n",
    "            if '.' in amount or ',' in amount:\n",
    "                amount_word = self._convert_decimal_number(amount)\n",
    "            else:\n",
    "                amount_word = self._number_to_words(int(amount))\n",
    "\n",
    "            unit_mappings = {\n",
    "                'кг': 'килограммов', 'г': 'граммов', 'км': 'километров',\n",
    "                'м': 'метров', 'см': 'сантиметров', 'мм': 'миллиметров',\n",
    "                'л': 'литров', 'мл': 'миллилитров', '°C': 'градусов цельсия',\n",
    "                '°': 'градусов'\n",
    "            }\n",
    "\n",
    "            unit_word = unit_mappings.get(unit, unit)\n",
    "            return f\"{amount_word} {unit_word}\"\n",
    "\n",
    "        text = re.sub(r'(\\d+(?:[.,]\\d+)?)\\s*(кг|г|км|м|см|мм|л|мл|°C|°)', convert_measurement, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_percentages(self, text: str) -> str:\n",
    "        \"\"\"Normalize percentage expressions.\"\"\"\n",
    "        def convert_percentage(match):\n",
    "            amount = match.group(1)\n",
    "            if '.' in amount or ',' in amount:\n",
    "                amount_word = self._convert_decimal_number(amount)\n",
    "            else:\n",
    "                amount_word = self._number_to_words(int(amount))\n",
    "            return f\"{amount_word} процентов\"\n",
    "\n",
    "        text = re.sub(r'(\\d+(?:[.,]\\d+)?)%', convert_percentage, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_phone_numbers(self, text: str) -> str:\n",
    "        \"\"\"Normalize phone numbers.\"\"\"\n",
    "        def convert_phone(match):\n",
    "            phone = match.group()\n",
    "            digits = re.sub(r'[^\\d]', '', phone)\n",
    "            spoken_digits = []\n",
    "            for digit in digits:\n",
    "                spoken_digits.append(self.numbers.get(digit, digit))\n",
    "            return ' '.join(spoken_digits)\n",
    "\n",
    "        phone_patterns = [\n",
    "            r'\\+7\\s*\\(\\d{3}\\)\\s*\\d{3}-\\d{2}-\\d{2}',\n",
    "            r'\\+7\\s*\\d{10}',\n",
    "            r'8\\s*\\(\\d{3}\\)\\s*\\d{3}-\\d{2}-\\d{2}',\n",
    "            r'8\\s*\\d{10}'\n",
    "        ]\n",
    "\n",
    "        for pattern in phone_patterns:\n",
    "            text = re.sub(pattern, convert_phone, text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _normalize_urls_emails(self, text: str) -> str:\n",
    "        \"\"\"Normalize URLs and email addresses.\"\"\"\n",
    "        text = re.sub(r'https?://[^\\s]+', 'ссылка', text)\n",
    "        text = re.sub(r'www\\.[^\\s]+', 'веб сайт', text)\n",
    "        text = re.sub(r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b', 'электронная почта', text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_abbreviations(self, text: str) -> str:\n",
    "        \"\"\"Normalize common abbreviations.\"\"\"\n",
    "        abbreviations = {\n",
    "            'т.е.': 'то есть', 'и т.д.': 'и так далее', 'и т.п.': 'и тому подобное',\n",
    "            'т.к.': 'так как', 'т.н.': 'так называемый', 'см.': 'смотрите',\n",
    "            'стр.': 'страница', 'гл.': 'глава', 'рис.': 'рисунок',\n",
    "            'табл.': 'таблица', 'г.': 'год', 'гг.': 'годы', 'в.': 'век',\n",
    "            'вв.': 'века', 'др.': 'другие', 'пр.': 'прочие',\n",
    "            'напр.': 'например', 'англ.': 'английский', 'рус.': 'русский'\n",
    "        }\n",
    "\n",
    "        for abbr, expansion in abbreviations.items():\n",
    "            text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', expansion, text, flags=re.IGNORECASE)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _normalize_roman_numerals(self, text: str) -> str:\n",
    "        \"\"\"Normalize Roman numerals.\"\"\"\n",
    "        def convert_roman(match):\n",
    "            roman = match.group()\n",
    "            return self.roman_numerals.get(roman, roman)\n",
    "\n",
    "        text = re.sub(r'\\b(I{1,3}|IV|V|VI{0,3}|IX|X{1,2}|XI{0,3}|XIV|XV|XVI{0,3}|XIX|XX)\\b', convert_roman, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_numbers(self, text: str) -> str:\n",
    "        \"\"\"Normalize standalone numbers to words.\"\"\"\n",
    "        def convert_number(match):\n",
    "            number = match.group()\n",
    "            try:\n",
    "                num = int(number)\n",
    "                return self._number_to_words(num)\n",
    "            except ValueError:\n",
    "                return number\n",
    "\n",
    "        text = re.sub(r'\\b\\d{1,4}\\b', convert_number, text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_punctuation(self, text: str) -> str:\n",
    "        \"\"\"Handle punctuation marks.\"\"\"\n",
    "        text = text.replace('&', ' и ')\n",
    "        text = text.replace('+', ' плюс ')\n",
    "        text = text.replace('=', ' равно ')\n",
    "        text = text.replace('№', 'номер ')\n",
    "\n",
    "        text = re.sub(r'[()[\\]{}]', '', text)\n",
    "        text = re.sub(r'[.!?]+', '.', text)\n",
    "        text = re.sub(r'[,;:]+', ',', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _postprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Final cleanup of normalized text.\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        text = re.sub(r'[.!?,;:]+$', '', text)\n",
    "        text = text.lower()\n",
    "        return text\n",
    "\n",
    "    def _number_to_words(self, num: int) -> str:\n",
    "        \"\"\"Convert numbers to Russian words.\"\"\"\n",
    "        if num == 0:\n",
    "            return 'ноль'\n",
    "\n",
    "        if 1 <= num <= 19:\n",
    "            return self.numbers[str(num)]\n",
    "        elif 20 <= num <= 99:\n",
    "            tens = (num // 10) * 10\n",
    "            units = num % 10\n",
    "            if units == 0:\n",
    "                return self.numbers[str(tens)]\n",
    "            else:\n",
    "                return f\"{self.numbers[str(tens)]} {self.numbers[str(units)]}\"\n",
    "        elif 100 <= num <= 999:\n",
    "            hundreds = (num // 100) * 100\n",
    "            remainder = num % 100\n",
    "            result = self.hundreds[str(hundreds)]\n",
    "            if remainder > 0:\n",
    "                result += f\" {self._number_to_words(remainder)}\"\n",
    "            return result\n",
    "        elif 1000 <= num <= 9999:\n",
    "            thousands = num // 1000\n",
    "            remainder = num % 1000\n",
    "\n",
    "            if thousands == 1:\n",
    "                result = \"тысяча\"\n",
    "            elif 2 <= thousands <= 4:\n",
    "                result = f\"{self.numbers[str(thousands)]} тысячи\"\n",
    "            else:\n",
    "                result = f\"{self.numbers[str(thousands)]} тысяч\"\n",
    "\n",
    "            if remainder > 0:\n",
    "                result += f\" {self._number_to_words(remainder)}\"\n",
    "            return result\n",
    "        else:\n",
    "            return str(num)\n",
    "\n",
    "    def _convert_decimal_number(self, number_str: str) -> str:\n",
    "        \"\"\"Convert decimal numbers to words.\"\"\"\n",
    "        number_str = number_str.replace(',', '.')\n",
    "\n",
    "        try:\n",
    "            if '.' in number_str:\n",
    "                integer_part, decimal_part = number_str.split('.')\n",
    "                integer_word = self._number_to_words(int(integer_part))\n",
    "                decimal_digits = ' '.join([self.numbers.get(digit, digit) for digit in decimal_part])\n",
    "                return f\"{integer_word} целых {decimal_digits}\"\n",
    "            else:\n",
    "                return self._number_to_words(int(number_str))\n",
    "        except ValueError:\n",
    "            return number_str\n",
    "\n",
    "    def _convert_year(self, year: str) -> str:\n",
    "        \"\"\"Convert year to spoken form.\"\"\"\n",
    "        try:\n",
    "            year_int = int(year)\n",
    "            if 1000 <= year_int <= 2099:\n",
    "                return self._number_to_words(year_int)\n",
    "            else:\n",
    "                return year\n",
    "        except ValueError:\n",
    "            return year\n",
    "\n",
    "    def _get_currency_word(self, currency: str) -> str:\n",
    "        \"\"\"Get proper currency word.\"\"\"\n",
    "        currency_map = {\n",
    "            '₽': 'рублей', 'руб': 'рублей', '$': 'долларов', '€': 'евро'\n",
    "        }\n",
    "        return currency_map.get(currency, currency)\n",
    "\n",
    "    def normalize_with_dict(self):\n",
    "        \"\"\"Normalize text using dictionary approach.\"\"\"\n",
    "        logger.info(\"Starting dictionary-based normalization...\")\n",
    "\n",
    "        # Load dictionary\n",
    "        try:\n",
    "            with open(self.dictionary_path, 'r', encoding='utf-8') as f:\n",
    "                dictionary = json.load(f)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load dictionary: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Load test data\n",
    "        try:\n",
    "            test = self.cleaner.safe_load_csv(self.test_path)\n",
    "            if test is None:\n",
    "                logger.error(\"Failed to load test data\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load test data: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Process test data\n",
    "        test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
    "        test['before_l'] = test['before'].str.lower()\n",
    "        test['after'] = test['before_l'].map(lambda x: dictionary.get(x, x))\n",
    "\n",
    "        def fix_case(original, lower, after):\n",
    "            if lower == after:\n",
    "                return original\n",
    "            else:\n",
    "                return after\n",
    "\n",
    "        test['after'] = test.apply(lambda r: fix_case(r['before'], r['before_l'], r['after']), axis=1)\n",
    "\n",
    "        return test\n",
    "\n",
    "    def normalize_with_neural(self, test_mode=False):\n",
    "        \"\"\"\n",
    "        Normalize text using neural T5 model.\n",
    "        Args:\n",
    "            test_mode (bool): If True, only process first 10 items for testing\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting neural normalization with T5 model...\")\n",
    "\n",
    "        try:\n",
    "            # Load test data\n",
    "            test = self.cleaner.safe_load_csv(self.test_path)\n",
    "            if test is None:\n",
    "                logger.error(\"Failed to load test data\")\n",
    "                return\n",
    "\n",
    "            test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
    "\n",
    "            if test_mode:\n",
    "                logger.info(\"Running in test mode - processing only first 10 elements\")\n",
    "                test = test.head(10)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load test data: {e}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Setup device and model\n",
    "            MODEL_NAME = \"saarus72/russian_text_normalizer\"\n",
    "            cache_dir = Path('models_cache')\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                logger.info(f\"CUDA available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                device = torch.device(\"cuda\")\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                logger.warning(\"CUDA not available. Using CPU.\")\n",
    "                device = torch.device(\"cpu\")\n",
    "\n",
    "            # Load model and tokenizer\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, cache_dir=cache_dir)\n",
    "            model = T5ForConditionalGeneration.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                cache_dir=cache_dir,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                low_cpu_mem_usage=True,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            logger.info(f\"Model loaded and running on {device}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            return\n",
    "\n",
    "        normalized_texts = []\n",
    "        batch_size = 256 if torch.cuda.is_available() else 32\n",
    "        if test_mode:\n",
    "            batch_size = 2\n",
    "\n",
    "        logger.info(f\"Batch size: {batch_size}\")\n",
    "\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                for i in tqdm(range(0, len(test), batch_size)):\n",
    "                    batch_texts = test['before'].iloc[i:i + batch_size].tolist()\n",
    "\n",
    "                    # Format inputs according to model requirements\n",
    "                    formatted_texts = []\n",
    "                    max_len = 0\n",
    "                    for text in batch_texts:\n",
    "                        if any(c.isdigit() or (c.isascii() and c.isalpha()) for c in text):\n",
    "                            if text.isdigit():\n",
    "                                text_rev = text[::-1]\n",
    "                                groups = [text_rev[i:i+3][::-1] for i in range(0, len(text_rev), 3)]\n",
    "                                text = ' '.join(groups[::-1])\n",
    "                            formatted_text = f\"<SC1>[{text}]<extra_id_0>\"\n",
    "                        else:\n",
    "                            formatted_text = text\n",
    "                        formatted_texts.append(formatted_text)\n",
    "                        max_len = max(max_len, len(formatted_text))\n",
    "\n",
    "                    # Tokenize with dynamic padding\n",
    "                    inputs = tokenizer(\n",
    "                        formatted_texts,\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=min(max_len + 10, 128),\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    input_ids = inputs[\"input_ids\"].to(device)\n",
    "                    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "                    # Generate with optimized parameters\n",
    "                    outputs = model.generate(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=min(max_len + 20, 128),\n",
    "                        num_beams=2,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=False,\n",
    "                        use_cache=True,\n",
    "                        eos_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "\n",
    "                    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "                    cleaned_outputs = []\n",
    "                    for output, original_text in zip(decoded_outputs, batch_texts):\n",
    "                        text = output.replace(\"<SC1>\", \"\").replace(\"<extra_id_0>\", \"\").strip()\n",
    "                        text = text.strip('[]')\n",
    "                        if not any(c.isdigit() or (c.isascii() and c.isalpha()) for c in original_text):\n",
    "                            text = original_text\n",
    "                        cleaned_outputs.append(text)\n",
    "\n",
    "                    normalized_texts.extend(cleaned_outputs)\n",
    "\n",
    "                # Clear GPU memory\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            # Create results DataFrame\n",
    "            results_df = pd.DataFrame({\n",
    "                'id': test['id'],\n",
    "                'after': normalized_texts\n",
    "            })\n",
    "\n",
    "            # Save results\n",
    "            output_path = self.result_path.replace('.csv', '_test.csv') if test_mode else self.result_path\n",
    "            logger.info(f\"Saving results to {output_path}\")\n",
    "            results_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "            logger.info(\"Neural normalization completed successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during neural normalization: {e}\")\n",
    "            return\n",
    "\n",
    "    def normalize_combined(self, test_mode=False):\n",
    "        \"\"\"\n",
    "        Combined normalization approach: dictionary first, then neural for remaining tokens.\n",
    "        Args:\n",
    "            test_mode (bool): If True, only process first 10 items for testing\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting combined normalization (dictionary + neural)...\")\n",
    "\n",
    "        # Step 1: Dictionary normalization\n",
    "        logger.info(\"Step 1: Applying dictionary normalization...\")\n",
    "        dict_results = self.normalize_with_dict()\n",
    "        if dict_results is None:\n",
    "            logger.error(\"Failed to perform dictionary normalization\")\n",
    "            return\n",
    "\n",
    "        # Identify tokens that need neural normalization\n",
    "        needs_neural = []\n",
    "        for idx, row in dict_results.iterrows():\n",
    "            # If after dictionary normalization text didn't change and contains digits or Latin letters\n",
    "            if (row['before'] == row['after'] and\n",
    "                any(c.isdigit() or (c.isascii() and c.isalpha()) for c in row['before'])):\n",
    "                needs_neural.append(idx)\n",
    "\n",
    "        logger.info(f\"Found {len(needs_neural)} tokens requiring neural normalization\")\n",
    "\n",
    "        if not needs_neural:\n",
    "            logger.info(\"All tokens successfully normalized with dictionary\")\n",
    "            output_path = self.result_path.replace('.csv', '_test.csv') if test_mode else self.result_path\n",
    "            dict_results[['id', 'after']].to_csv(output_path, index=False, encoding='utf-8')\n",
    "            return\n",
    "\n",
    "        # Step 2: Neural normalization for remaining tokens\n",
    "        logger.info(\"Step 2: Applying neural normalization for remaining tokens...\")\n",
    "\n",
    "        try:\n",
    "            # Setup neural model (same as in normalize_with_neural)\n",
    "            MODEL_NAME = \"saarus72/russian_text_normalizer\"\n",
    "            cache_dir = Path('models_cache')\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                logger.info(f\"CUDA available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                device = torch.device(\"cuda\")\n",
    "                torch.cuda.empty_cache()\n",
    "            else:\n",
    "                logger.warning(\"CUDA not available. Using CPU.\")\n",
    "                device = torch.device(\"cpu\")\n",
    "\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, cache_dir=cache_dir)\n",
    "            model = T5ForConditionalGeneration.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                cache_dir=cache_dir,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                low_cpu_mem_usage=True,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            # Process only tokens that need neural normalization\n",
    "            neural_texts = dict_results.iloc[needs_neural]\n",
    "            batch_size = 256 if torch.cuda.is_available() else 32\n",
    "            if test_mode:\n",
    "                batch_size = 2\n",
    "\n",
    "            logger.info(f\"Batch size: {batch_size}\")\n",
    "            normalized_neural = []\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                for i in tqdm(range(0, len(neural_texts), batch_size)):\n",
    "                    batch_texts = neural_texts['before'].iloc[i:i + batch_size].tolist()\n",
    "\n",
    "                    formatted_texts = []\n",
    "                    max_len = 0\n",
    "                    for text in batch_texts:\n",
    "                        if text.isdigit():\n",
    "                            text_rev = text[::-1]\n",
    "                            groups = [text_rev[i:i+3][::-1] for i in range(0, len(text_rev), 3)]\n",
    "                            text = ' '.join(groups[::-1])\n",
    "                        formatted_text = f\"<SC1>[{text}]<extra_id_0>\"\n",
    "                        formatted_texts.append(formatted_text)\n",
    "                        max_len = max(max_len, len(formatted_text))\n",
    "\n",
    "                    inputs = tokenizer(\n",
    "                        formatted_texts,\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=min(max_len + 10, 128),\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    input_ids = inputs[\"input_ids\"].to(device)\n",
    "                    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "                    outputs = model.generate(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=min(max_len + 20, 128),\n",
    "                        num_beams=2,\n",
    "                        early_stopping=True,\n",
    "                        do_sample=False,\n",
    "                        use_cache=True,\n",
    "                        eos_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "\n",
    "                    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "                    cleaned_outputs = []\n",
    "                    for output in decoded_outputs:\n",
    "                        text = output.replace(\"<SC1>\", \"\").replace(\"<extra_id_0>\", \"\").strip()\n",
    "                        text = text.strip('[]')\n",
    "                        cleaned_outputs.append(text)\n",
    "\n",
    "                    normalized_neural.extend(cleaned_outputs)\n",
    "\n",
    "            # Update results with neural normalization\n",
    "            for idx, neural_text in zip(needs_neural, normalized_neural):\n",
    "                dict_results.at[idx, 'after'] = neural_text\n",
    "\n",
    "            # Save final results\n",
    "            output_path = self.result_path.replace('.csv', '_test.csv') if test_mode else self.result_path\n",
    "            logger.info(f\"Saving results to {output_path}\")\n",
    "            dict_results[['id', 'after']].to_csv(output_path, index=False, encoding='utf-8')\n",
    "            logger.info(\"Combined normalization completed successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during neural normalization: {e}\")\n",
    "            return\n",
    "\n",
    "    def create_submission(self):\n",
    "        \"\"\"Create final submission file.\"\"\"\n",
    "        logger.info(\"Creating submission file...\")\n",
    "\n",
    "        try:\n",
    "            # Check if result file exists\n",
    "            if os.path.exists(self.result_path):\n",
    "                df = pd.read_csv(self.result_path, encoding='utf-8')\n",
    "                logger.info(f\"Loaded results with {len(df)} rows\")\n",
    "\n",
    "                # Ensure proper format for submission\n",
    "                if 'id' in df.columns and 'after' in df.columns:\n",
    "                    submission_df = df[['id', 'after']].copy()\n",
    "                    submission_path = self.result_path.replace('.csv', '_submission.csv')\n",
    "                    submission_df.to_csv(submission_path, index=False, encoding='utf-8')\n",
    "                    logger.info(f\"Submission file created: {submission_path}\")\n",
    "                    return submission_path\n",
    "                else:\n",
    "                    logger.error(\"Result file missing required columns 'id' and 'after'\")\n",
    "                    return None\n",
    "            else:\n",
    "                logger.error(f\"Result file not found: {self.result_path}\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating submission file: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the enhanced text normalization.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Enhanced Russian Text Normalization\")\n",
    "    parser.add_argument(\"mode\", choices=[\"train\", \"normalize\"],\n",
    "                       help=\"Operation mode: train or normalize\")\n",
    "    parser.add_argument(\"method\", choices=[\"dictionary\", \"neural\", \"combined\", \"rules\"],\n",
    "                       help=\"Method: dictionary, neural, combined, or rules\")\n",
    "    parser.add_argument(\"--test\", action=\"store_true\",\n",
    "                       help=\"Run in test mode (only 10 elements)\")\n",
    "    parser.add_argument(\"--create-submission\", action=\"store_true\",\n",
    "                       help=\"Create submission file after normalization\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Initialize the normalizer\n",
    "    normalizer = AdvancedRussianTextNormalizer()\n",
    "\n",
    "    if args.mode == \"train\":\n",
    "        if args.method == \"dictionary\":\n",
    "            normalizer.train_enhanced_dict()\n",
    "        else:\n",
    "            logger.info(f\"Training for {args.method} method not implemented yet\")\n",
    "\n",
    "    elif args.mode == \"normalize\":\n",
    "        if args.method == \"dictionary\":\n",
    "            result = normalizer.normalize_with_dict()\n",
    "            if result is not None:\n",
    "                output_path = normalizer.result_path.replace('.csv', '_test.csv') if args.test else normalizer.result_path\n",
    "                result[['id', 'after']].to_csv(output_path, index=False, encoding='utf-8')\n",
    "                logger.info(f\"Dictionary normalization results saved to {output_path}\")\n",
    "\n",
    "        elif args.method == \"neural\":\n",
    "            normalizer.normalize_with_neural(test_mode=args.test)\n",
    "\n",
    "        elif args.method == \"combined\":\n",
    "            normalizer.normalize_combined(test_mode=args.test)\n",
    "\n",
    "        elif args.method == \"rules\":\n",
    "            # For rule-based normalization, we need to process the test file\n",
    "            try:\n",
    "                test = normalizer.cleaner.safe_load_csv(normalizer.test_path)\n",
    "                if test is not None:\n",
    "                    test['id'] = test['sentence_id'].astype(str) + '_' + test['token_id'].astype(str)\n",
    "                    if args.test:\n",
    "                        test = test.head(10)\n",
    "\n",
    "                    test['after'] = test['before'].apply(normalizer.normalize_with_rules)\n",
    "\n",
    "                    output_path = normalizer.result_path.replace('.csv', '_test.csv') if args.test else normalizer.result_path\n",
    "                    test[['id', 'after']].to_csv(output_path, index=False, encoding='utf-8')\n",
    "                    logger.info(f\"Rule-based normalization results saved to {output_path}\")\n",
    "                else:\n",
    "                    logger.error(\"Failed to load test data\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in rule-based normalization: {e}\")\n",
    "\n",
    "        # Create submission file if requested\n",
    "        if args.create_submission:\n",
    "            submission_path = normalizer.create_submission()\n",
    "            if submission_path:\n",
    "                logger.info(f\"Submission file ready: {submission_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--test] [--create-submission]\n",
      "                             {train,normalize}\n",
      "                             {dictionary,neural,combined,rules}\n",
      "ipykernel_launcher.py: error: argument mode: invalid choice: '/Users/nikitaskazutin/Library/Jupyter/runtime/kernel-f7545934-c6f3-48ea-9611-7586d79e2b06.json' (choose from train, normalize)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[31mSystemExit\u001B[39m\u001B[31m:\u001B[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaskazutin/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T05:10:43.703197Z",
     "start_time": "2025-06-01T05:10:40.365559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Quick submission creation script for Russian text normalization.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_submission_from_results(result_file='data/results/result.csv',\n",
    "                                 output_file='data/results/submission.csv'):\n",
    "    \"\"\"\n",
    "    Create a properly formatted submission file from results.\n",
    "\n",
    "    Args:\n",
    "        result_file: Path to the result file with normalization outputs\n",
    "        output_file: Path where submission file should be saved\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load results\n",
    "        if not os.path.exists(result_file):\n",
    "            logger.error(f\"Result file not found: {result_file}\")\n",
    "            return False\n",
    "\n",
    "        df = pd.read_csv(result_file, encoding='utf-8')\n",
    "        logger.info(f\"Loaded results with {len(df)} rows\")\n",
    "\n",
    "        # Validate columns\n",
    "        required_cols = ['id', 'after']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            logger.error(f\"Missing required columns: {missing_cols}\")\n",
    "            return False\n",
    "\n",
    "        # Create submission format\n",
    "        submission_df = df[['id', 'after']].copy()\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "        # Save submission\n",
    "        submission_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Submission file created: {output_file}\")\n",
    "        logger.info(f\"Submission contains {len(submission_df)} entries\")\n",
    "\n",
    "        # Show sample entries\n",
    "        logger.info(\"Sample entries:\")\n",
    "        for i, row in submission_df.head(3).iterrows():\n",
    "            logger.info(f\"  {row['id']}: {row['after']}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating submission: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Create submission file from results\")\n",
    "    parser.add_argument(\"--input\", default=\"data/results/result.csv\",\n",
    "                       help=\"Input result file (default: data/results/result.csv)\")\n",
    "    parser.add_argument(\"--output\", default=\"data/results/submission.csv\",\n",
    "                       help=\"Output submission file (default: data/results/submission.csv)\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    success = create_submission_from_results(args.input, args.output)\n",
    "    if success:\n",
    "        print(f\"✅ Submission file ready: {args.output}\")\n",
    "    else:\n",
    "        print(\"❌ Failed to create submission file\")"
   ],
   "id": "8c1cb250f61fdeec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitaskazutin/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T04:34:00.940832Z",
     "start_time": "2025-06-01T04:34:00.920126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "if __name__ == '__main__':\n",
    "    import multiprocessing\n",
    "    multiprocessing.set_start_method('fork', force=True)\n",
    "\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "    torch.set_num_threads(8)\n",
    "\n",
    "    DATA_PATH = 'data/to_normalize.csv'\n",
    "    OUT_PATH = 'data/normalized_llm.csv'\n",
    "    model = fast_training_pipeline(\"data/ru_train.csv\")"
   ],
   "id": "637357b3233542b9",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T05:10:49.451960Z",
     "start_time": "2025-06-01T05:10:49.448129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import psutil\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.step_times = []\n",
    "\n",
    "    def start_epoch(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def log_step(self, step, loss):\n",
    "        current_time = time.time()\n",
    "        if self.start_time:\n",
    "            step_time = current_time - self.start_time\n",
    "            self.step_times.append(step_time)\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                avg_step_time = sum(self.step_times[-50:]) / len(self.step_times[-50:])\n",
    "                memory_usage = psutil.virtual_memory().percent\n",
    "                print(f\"Step {step}: Loss={loss:.4f}, Avg Step Time={avg_step_time:.2f}s, Memory={memory_usage:.1f}%\")\n",
    "\n",
    "        self.start_time = current_time"
   ],
   "id": "f1b1d8045427da33",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T05:42:00.648163Z",
     "start_time": "2025-06-01T05:42:00.609565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('./data/log_file.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Specify the pre-trained model\n",
    "MODEL_NAME = \"saarus72/russian_text_normalizer\"\n",
    "\n",
    "class EnhancedTextNormalizationEnsemble:\n",
    "    \"\"\"\n",
    "    Enhanced ensemble for Russian text normalization combining dictionary,\n",
    "    rule-based, and T5 approaches based on successful Kaggle solutions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rule_model):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced ensemble.\n",
    "\n",
    "        Args:\n",
    "            rule_model: Instance of My_TextNormalization_Model for rule-based normalization\n",
    "        \"\"\"\n",
    "        self.rule_model = rule_model\n",
    "        self.dictionary = {}\n",
    "        self.cache = {}  # Cache for normalization results\n",
    "\n",
    "        # Create dictionary from training data if available\n",
    "        self._create_dictionary()\n",
    "\n",
    "        # Setup model\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.device = None\n",
    "\n",
    "        # Create cache directory for model\n",
    "        os.makedirs('models_cache', exist_ok=True)\n",
    "\n",
    "    def _create_dictionary(self):\n",
    "        \"\"\"Create dictionary from training data for fast lookups.\"\"\"\n",
    "        dictionary_path = 'data/dictionary/model_dictionary.json'\n",
    "\n",
    "        # Try to load existing dictionary\n",
    "        if os.path.exists(dictionary_path):\n",
    "            try:\n",
    "                with open(dictionary_path, 'r', encoding='utf-8') as f:\n",
    "                    self.dictionary = json.load(f)\n",
    "                logger.info(f\"Loaded dictionary with {len(self.dictionary)} entries\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load dictionary: {str(e)}\")\n",
    "\n",
    "        # Create dictionary if not loaded\n",
    "        try:\n",
    "            train_path = 'data/inputs/ru_train.csv'\n",
    "            if os.path.exists(train_path):\n",
    "                logger.info(\"Creating dictionary from training data...\")\n",
    "                train = pd.read_csv(train_path, encoding='utf-8')\n",
    "                train['before'] = train['before'].str.lower()\n",
    "                train['after'] = train['after'].str.lower()\n",
    "\n",
    "                # Group by before and after, counting occurrences\n",
    "                train = train.groupby(['before', 'after'], as_index=False)['sentence_id'].count()\n",
    "                train = train.sort_values(['sentence_id', 'before'], ascending=[False, True])\n",
    "                train = train.drop_duplicates(['before'])\n",
    "\n",
    "                self.dictionary = {key: value for (key, value) in train[['before', 'after']].values}\n",
    "                logger.info(f\"Created dictionary with {len(self.dictionary)} entries\")\n",
    "\n",
    "                # Save dictionary\n",
    "                os.makedirs(os.path.dirname(dictionary_path), exist_ok=True)\n",
    "                with open(dictionary_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(self.dictionary, f, indent=4, ensure_ascii=False)\n",
    "                logger.info(f\"Dictionary saved to {dictionary_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating dictionary: {str(e)}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the T5 model for normalization.\"\"\"\n",
    "        if self.model is not None:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Loading model and tokenizer...\")\n",
    "\n",
    "            # Set device\n",
    "            if torch.cuda.is_available():\n",
    "                logger.info(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "                self.device = torch.device(\"cuda\")\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.backends.cudnn.benchmark = True\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "                logger.info(\"Using MPS (Apple Silicon)\")\n",
    "                self.device = torch.device(\"mps\")\n",
    "            else:\n",
    "                logger.info(\"Using CPU\")\n",
    "                self.device = torch.device(\"cpu\")\n",
    "\n",
    "            # Load tokenizer and model\n",
    "            cache_dir = Path('models_cache')\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, cache_dir=cache_dir)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                MODEL_NAME,\n",
    "                cache_dir=cache_dir,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                low_cpu_mem_usage=True,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            logger.info(\"Model loaded successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {str(e)}\")\n",
    "            self.model = None\n",
    "            self.tokenizer = None\n",
    "\n",
    "    def _normalize_with_dictionary(self, text):\n",
    "        \"\"\"Normalize text using dictionary.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        if text_lower in self.dictionary:\n",
    "            return self.dictionary[text_lower]\n",
    "        return None\n",
    "\n",
    "    def _normalize_with_rules(self, text):\n",
    "        \"\"\"Normalize text using rule-based model.\"\"\"\n",
    "        # Use cache to avoid repeated normalizations\n",
    "        if text in self.cache:\n",
    "            return self.cache[text]\n",
    "\n",
    "        # Disable frequent logging\n",
    "        original_level = logger.level\n",
    "        try:\n",
    "            logger.setLevel(logging.ERROR)  # Temporarily set to ERROR level\n",
    "            result = self.rule_model.normalize_text(text)\n",
    "            self.cache[text] = result\n",
    "            return result\n",
    "        finally:\n",
    "            logger.setLevel(original_level)\n",
    "\n",
    "    def _normalize_with_t5(self, texts):\n",
    "        \"\"\"\n",
    "        Normalize a batch of texts using the T5 model.\n",
    "\n",
    "        Args:\n",
    "            texts: List of texts to normalize\n",
    "\n",
    "        Returns:\n",
    "            List of normalized texts\n",
    "        \"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            self.load_model()\n",
    "\n",
    "        if self.model is None:\n",
    "            # If model still not loaded, use rules\n",
    "            return [self._normalize_with_rules(text) for text in texts]\n",
    "\n",
    "        try:\n",
    "            # Format inputs according to model requirements\n",
    "            formatted_texts = []\n",
    "            max_len = 0\n",
    "            for text in texts:\n",
    "                # Special handling for numbers (format with spaces for easier processing)\n",
    "                if text.isdigit():\n",
    "                    text_rev = text[::-1]\n",
    "                    groups = [text_rev[i:i+3][::-1] for i in range(0, len(text_rev), 3)]\n",
    "                    text = ' '.join(groups[::-1])\n",
    "                formatted_text = f\"<SC1>[{text}]<extra_id_0>\"\n",
    "                formatted_texts.append(formatted_text)\n",
    "                max_len = max(max_len, len(formatted_text))\n",
    "\n",
    "            # Tokenize with dynamic padding\n",
    "            inputs = self.tokenizer(\n",
    "                formatted_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=min(max_len + 10, 128),\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "            # Generate\n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=min(max_len + 20, 128),\n",
    "                    num_beams=2,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False,\n",
    "                    use_cache=True,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            decoded_outputs = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "            # Clean outputs\n",
    "            cleaned_outputs = []\n",
    "            for output in decoded_outputs:\n",
    "                text = output.replace(\"<SC1>\", \"\").replace(\"<extra_id_0>\", \"\").strip()\n",
    "                text = text.strip('[]')\n",
    "                cleaned_outputs.append(text)\n",
    "\n",
    "            return cleaned_outputs\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in T5 normalization: {str(e)}\")\n",
    "            return [self._normalize_with_rules(text) for text in texts]\n",
    "\n",
    "    def _needs_neural(self, text):\n",
    "        \"\"\"Determine if text needs neural normalization.\"\"\"\n",
    "        # Check if text contains digits or specific patterns\n",
    "        if any(c.isdigit() for c in text):\n",
    "            return True\n",
    "\n",
    "        # Check for English text\n",
    "        if any(c.isascii() and c.isalpha() for c in text):\n",
    "            return True\n",
    "\n",
    "        # Check for special patterns\n",
    "        patterns = [\n",
    "            r'\\b\\d+\\b',  # Numbers\n",
    "            r'\\b\\d+[.,]\\d+\\b',  # Decimal numbers\n",
    "            r'\\b(?:\\+7|8)[\\s\\-]?\\(?(?:\\d{3})\\)?[\\s\\-]?\\d{3}[\\s\\-]?\\d{2}[\\s\\-]?\\d{2}\\b',  # Phone numbers\n",
    "            r'\\b(?:https?://|www\\.)[^\\s]+',  # URLs\n",
    "            r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b',  # Emails\n",
    "            r'\\b[IVXLCDM]+\\b'  # Roman numerals\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, text):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        \"\"\"\n",
    "        Normalize a single text using the ensemble approach.\n",
    "\n",
    "        Args:\n",
    "            text: Text to normalize\n",
    "\n",
    "        Returns:\n",
    "            Normalized text\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Try dictionary first (fastest)\n",
    "        dict_result = self._normalize_with_dictionary(text)\n",
    "        if dict_result is not None:\n",
    "            return dict_result\n",
    "\n",
    "        # If not in dictionary, decide between rules and neural\n",
    "        if self._needs_neural(text):\n",
    "            # For single text, still process as batch of 1\n",
    "            neural_results = self._normalize_with_t5([text])\n",
    "            return neural_results[0]\n",
    "        else:\n",
    "            return self._normalize_with_rules(text)\n",
    "\n",
    "    def normalize_batch(self, texts, batch_size=256):\n",
    "        \"\"\"\n",
    "        Normalize a batch of texts.\n",
    "\n",
    "        Args:\n",
    "            texts: List of texts to normalize\n",
    "            batch_size: Batch size for neural processing\n",
    "\n",
    "        Returns:\n",
    "            List of normalized texts\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # First try dictionary for all texts\n",
    "        dict_results = []\n",
    "        neural_needed_indices = []\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            dict_result = self._normalize_with_dictionary(text.lower())\n",
    "            if dict_result is not None:\n",
    "                dict_results.append(dict_result)\n",
    "            else:\n",
    "                dict_results.append(None)\n",
    "                if self._needs_neural(text):\n",
    "                    neural_needed_indices.append(i)\n",
    "                else:\n",
    "                    dict_results[i] = self._normalize_with_rules(text)\n",
    "\n",
    "        # Process neural batches if needed\n",
    "        if neural_needed_indices:\n",
    "            neural_texts = [texts[i] for i in neural_needed_indices]\n",
    "\n",
    "            # Process in batches\n",
    "            neural_results = []\n",
    "            for i in range(0, len(neural_texts), batch_size):\n",
    "                batch = neural_texts[i:i+batch_size]\n",
    "                batch_results = self._normalize_with_t5(batch)\n",
    "                neural_results.extend(batch_results)\n",
    "\n",
    "            # Update results\n",
    "            for idx, result in zip(neural_needed_indices, neural_results):\n",
    "                dict_results[idx] = result\n",
    "\n",
    "        return dict_results\n",
    "\n",
    "    def create_submission(self, test_df, output_path=\"submission.csv\"):\n",
    "        \"\"\"\n",
    "        Create a submission file for Kaggle.\n",
    "\n",
    "        Args:\n",
    "            test_df: DataFrame with test data\n",
    "            output_path: Path to save the submission file\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting submission creation with {len(test_df)} rows\")\n",
    "\n",
    "        # Check if required columns exist\n",
    "        required_columns = ['sentence_id', 'token_id', 'before']\n",
    "        for col in required_columns:\n",
    "            if col not in test_df.columns:\n",
    "                raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "        # Create ID column in the format \"sentence_id_token_id\"\n",
    "        test_df['id'] = test_df['sentence_id'].astype(str) + '_' + test_df['token_id'].astype(str)\n",
    "\n",
    "        # Process in batches by sentence for better efficiency\n",
    "        results = []\n",
    "        total_sentences = len(test_df['sentence_id'].unique())\n",
    "\n",
    "        for sentence_id, group in tqdm(test_df.groupby('sentence_id'), desc=f\"Processing {total_sentences} sentences\"):\n",
    "            # Get all \"before\" texts for this sentence\n",
    "            texts = group['before'].tolist()\n",
    "\n",
    "            # Normalize as a batch\n",
    "            normalized_texts = self.normalize_batch(texts)\n",
    "\n",
    "            # Add to results\n",
    "            for idx, row in group.iterrows():\n",
    "                results.append({\n",
    "                    'id': row['id'],\n",
    "                    'after': normalized_texts[idx - group.index[0]]  # Adjust index\n",
    "                })\n",
    "\n",
    "            # Log progress periodically\n",
    "            if sentence_id % 1000 == 0:\n",
    "                logger.info(f\"Processed {sentence_id}/{total_sentences} sentences\")\n",
    "\n",
    "        # Create submission DataFrame\n",
    "        submission_df = pd.DataFrame(results)\n",
    "        submission_df.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Submission saved to {output_path}\")\n",
    "\n",
    "        return submission_df"
   ],
   "id": "c8422e4877cd89f6",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T05:58:28.061796Z",
     "start_time": "2025-06-01T05:42:34.706992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"Run the enhanced ensemble normalization and create submission.\"\"\"\n",
    "    # Create the rule-based model\n",
    "    rule_model = My_TextNormalization_Model()\n",
    "\n",
    "    # Create the enhanced ensemble\n",
    "    logger.info(\"Creating enhanced ensemble...\")\n",
    "    ensemble = EnhancedTextNormalizationEnsemble(rule_model)\n",
    "\n",
    "    # Load the test data\n",
    "    logger.info(\"Loading test data...\")\n",
    "    test_df = pd.read_csv(\"./data/ru_test_2.csv\")\n",
    "\n",
    "    # Create submission\n",
    "    logger.info(\"Creating submission...\")\n",
    "    submission_df = ensemble.create_submission(test_df, \"enhanced_submission.csv\")\n",
    "\n",
    "    logger.info(\"Done!\")\n",
    "\n",
    "    # Test on a few examples\n",
    "    test_cases = [\n",
    "        \"15.03.2024\",\n",
    "        \"25₽\",\n",
    "        \"2,5кг\",\n",
    "        \"14:30\",\n",
    "        \"+7(495)123-45-67\",\n",
    "        \"1500\",\n",
    "        \"XX\"\n",
    "    ]\n",
    "\n",
    "    logger.info(\"Testing on examples:\")\n",
    "    for test in test_cases:\n",
    "        normalized = ensemble.normalize_text(test)\n",
    "        logger.info(f\"Original: {test}, Normalized: {normalized}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "43bfd99997c399c3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 12:42:34,710 - __main__ - INFO - Initializing Improved Russian Text Normalization Model\n",
      "2025-06-01 12:42:34,711 - __main__ - INFO - Improved Russian Text Normalization Model initialized successfully\n",
      "2025-06-01 12:42:34,712 - __main__ - INFO - Creating enhanced ensemble...\n",
      "2025-06-01 12:42:34,712 - __main__ - INFO - Loading test data...\n",
      "2025-06-01 12:42:34,984 - __main__ - INFO - Creating submission...\n",
      "2025-06-01 12:42:34,984 - __main__ - INFO - Starting submission creation with 989880 rows\n",
      "Processing 70000 sentences:   0%|          | 0/70000 [00:00<?, ?it/s]2025-06-01 12:42:35,764 - __main__ - INFO - Processed 0/70000 sentences\n",
      "2025-06-01 12:42:35,765 - __main__ - INFO - Loading model and tokenizer...\n",
      "2025-06-01 12:42:35,765 - __main__ - INFO - Using MPS (Apple Silicon)\n",
      "2025-06-01 12:47:51,607 - __main__ - INFO - Model loaded successfully\n",
      "Processing 70000 sentences:   1%|▏         | 1000/70000 [11:03<8:04:55,  2.37it/s] 2025-06-01 12:53:39,518 - __main__ - INFO - Processed 1000/70000 sentences\n",
      "Processing 70000 sentences:   2%|▏         | 1652/70000 [15:49<10:54:55,  1.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 37\u001B[39m\n\u001B[32m     34\u001B[39m         logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mOriginal: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, Normalized: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnormalized\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Create submission\u001B[39;00m\n\u001B[32m     15\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mCreating submission...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m submission_df = \u001B[43mensemble\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_submission\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43menhanced_submission.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mDone!\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# Test on a few examples\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 353\u001B[39m, in \u001B[36mEnhancedTextNormalizationEnsemble.create_submission\u001B[39m\u001B[34m(self, test_df, output_path)\u001B[39m\n\u001B[32m    350\u001B[39m texts = group[\u001B[33m'\u001B[39m\u001B[33mbefore\u001B[39m\u001B[33m'\u001B[39m].tolist()\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# Normalize as a batch\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m normalized_texts = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnormalize_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[38;5;66;03m# Add to results\u001B[39;00m\n\u001B[32m    356\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m idx, row \u001B[38;5;129;01min\u001B[39;00m group.iterrows():\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 316\u001B[39m, in \u001B[36mEnhancedTextNormalizationEnsemble.normalize_batch\u001B[39m\u001B[34m(self, texts, batch_size)\u001B[39m\n\u001B[32m    314\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(neural_texts), batch_size):\n\u001B[32m    315\u001B[39m     batch = neural_texts[i:i+batch_size]\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     batch_results = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_normalize_with_t5\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    317\u001B[39m     neural_results.extend(batch_results)\n\u001B[32m    319\u001B[39m \u001B[38;5;66;03m# Update results\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 202\u001B[39m, in \u001B[36mEnhancedTextNormalizationEnsemble._normalize_with_t5\u001B[39m\u001B[34m(self, texts)\u001B[39m\n\u001B[32m    200\u001B[39m \u001B[38;5;66;03m# Generate\u001B[39;00m\n\u001B[32m    201\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.inference_mode():\n\u001B[32m--> \u001B[39m\u001B[32m202\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    203\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    204\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    205\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mmin\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmax_len\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m128\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    206\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_beams\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    207\u001B[39m \u001B[43m        \u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    208\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdo_sample\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    209\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    210\u001B[39m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token_id\u001B[49m\n\u001B[32m    211\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    213\u001B[39m decoded_outputs = \u001B[38;5;28mself\u001B[39m.tokenizer.batch_decode(outputs, skip_special_tokens=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    215\u001B[39m \u001B[38;5;66;03m# Clean outputs\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/transformers/generation/utils.py:2616\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001B[39m\n\u001B[32m   2609\u001B[39m     input_ids, model_kwargs = \u001B[38;5;28mself\u001B[39m._expand_inputs_for_generation(\n\u001B[32m   2610\u001B[39m         input_ids=input_ids,\n\u001B[32m   2611\u001B[39m         expand_size=generation_config.num_beams,\n\u001B[32m   2612\u001B[39m         is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   2613\u001B[39m         **model_kwargs,\n\u001B[32m   2614\u001B[39m     )\n\u001B[32m   2615\u001B[39m     \u001B[38;5;66;03m# 12. run beam sample\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2616\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_beam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2617\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2618\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2619\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2620\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2621\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2622\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2623\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2625\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001B[32m   2626\u001B[39m     \u001B[38;5;66;03m# 11. prepare beam search scorer\u001B[39;00m\n\u001B[32m   2627\u001B[39m     beam_scorer = BeamSearchScorer(\n\u001B[32m   2628\u001B[39m         batch_size=batch_size,\n\u001B[32m   2629\u001B[39m         num_beams=generation_config.num_beams,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2635\u001B[39m         max_length=generation_config.max_length,\n\u001B[32m   2636\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/transformers/generation/utils.py:4030\u001B[39m, in \u001B[36mGenerationMixin._beam_search\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001B[39m\n\u001B[32m   4027\u001B[39m model_inputs.update({\u001B[33m\"\u001B[39m\u001B[33moutput_attentions\u001B[39m\u001B[33m\"\u001B[39m: output_attentions} \u001B[38;5;28;01mif\u001B[39;00m output_attentions \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[32m   4028\u001B[39m model_inputs.update({\u001B[33m\"\u001B[39m\u001B[33moutput_hidden_states\u001B[39m\u001B[33m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[32m-> \u001B[39m\u001B[32m4030\u001B[39m model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   4032\u001B[39m \u001B[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001B[39;00m\n\u001B[32m   4033\u001B[39m model_kwargs = \u001B[38;5;28mself\u001B[39m._update_model_kwargs_for_generation(\n\u001B[32m   4034\u001B[39m     model_outputs,\n\u001B[32m   4035\u001B[39m     model_kwargs,\n\u001B[32m   4036\u001B[39m     is_encoder_decoder=\u001B[38;5;28mself\u001B[39m.config.is_encoder_decoder,\n\u001B[32m   4037\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py:1811\u001B[39m, in \u001B[36mT5ForConditionalGeneration.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m   1808\u001B[39m         decoder_attention_mask = decoder_attention_mask.to(\u001B[38;5;28mself\u001B[39m.decoder.first_device)\n\u001B[32m   1810\u001B[39m \u001B[38;5;66;03m# Decode\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1811\u001B[39m decoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1812\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1813\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1814\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1815\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1816\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1817\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1818\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1819\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcross_attn_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1820\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1821\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1822\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1823\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1824\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1825\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1827\u001B[39m sequence_output = decoder_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1829\u001B[39m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py:1124\u001B[39m, in \u001B[36mT5Stack.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m   1107\u001B[39m     layer_outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m   1108\u001B[39m         layer_module.forward,\n\u001B[32m   1109\u001B[39m         hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1121\u001B[39m         cache_position,\n\u001B[32m   1122\u001B[39m     )\n\u001B[32m   1123\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1124\u001B[39m     layer_outputs = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1125\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1126\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1127\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1128\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1129\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1130\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_decoder_position_bias\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_decoder_position_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1131\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1132\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcross_attn_layer_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcross_attn_layer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1133\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1134\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1135\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1136\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1137\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1138\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1140\u001B[39m \u001B[38;5;66;03m# layer_outputs is a tuple with:\u001B[39;00m\n\u001B[32m   1141\u001B[39m \u001B[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001B[39;00m\n\u001B[32m   1142\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py:703\u001B[39m, in \u001B[36mT5Block.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001B[39m\n\u001B[32m    701\u001B[39m do_cross_attention = \u001B[38;5;28mself\u001B[39m.is_decoder \u001B[38;5;129;01mand\u001B[39;00m encoder_hidden_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    702\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m do_cross_attention:\n\u001B[32m--> \u001B[39m\u001B[32m703\u001B[39m     cross_attention_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    704\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    705\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_value_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    706\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    707\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_decoder_position_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    708\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcross_attn_layer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    709\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    710\u001B[39m \u001B[43m        \u001B[49m\u001B[43mquery_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m[\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    711\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    712\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    713\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    714\u001B[39m     hidden_states, past_key_value = cross_attention_outputs[:\u001B[32m2\u001B[39m]\n\u001B[32m    716\u001B[39m     \u001B[38;5;66;03m# clamp inf values to enable fp16 training\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py:633\u001B[39m, in \u001B[36mT5LayerCrossAttention.forward\u001B[39m\u001B[34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions, cache_position)\u001B[39m\n\u001B[32m    619\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    620\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    621\u001B[39m     hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m    630\u001B[39m     cache_position=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    631\u001B[39m ):\n\u001B[32m    632\u001B[39m     normed_hidden_states = \u001B[38;5;28mself\u001B[39m.layer_norm(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m633\u001B[39m     attention_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mEncDecAttention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    634\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnormed_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    635\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    636\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_value_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkey_value_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    637\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    638\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    639\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    640\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    641\u001B[39m \u001B[43m        \u001B[49m\u001B[43mquery_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquery_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    642\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    643\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    644\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    645\u001B[39m     layer_output = hidden_states + \u001B[38;5;28mself\u001B[39m.dropout(attention_output[\u001B[32m0\u001B[39m])\n\u001B[32m    646\u001B[39m     outputs = (layer_output,) + attention_output[\u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Library/Caches/pypoetry/virtualenvs/pythonproject-J-Nu3-4q-py3.13/lib/python3.13/site-packages/transformers/models/t5/modeling_t5.py:531\u001B[39m, in \u001B[36mT5Attention.forward\u001B[39m\u001B[34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001B[39m\n\u001B[32m    529\u001B[39m real_seq_length = query_length \u001B[38;5;28;01mif\u001B[39;00m query_length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m cache_position[-\u001B[32m1\u001B[39m] + \u001B[32m1\u001B[39m\n\u001B[32m    530\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.has_relative_attention_bias:\n\u001B[32m--> \u001B[39m\u001B[32m531\u001B[39m     position_bias = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    532\u001B[39m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_length\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscores\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscores\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdtype\u001B[49m\n\u001B[32m    533\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    534\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.gradient_checkpointing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.training:\n\u001B[32m    535\u001B[39m         position_bias.requires_grad = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T06:31:29.580671Z",
     "start_time": "2025-06-01T06:31:16.842131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"Run Russian text normalization and create Kaggle submission.\"\"\"\n",
    "    # Create normalizer\n",
    "    logger.info(\"Creating Kaggle-style Russian normalizer...\")\n",
    "    normalizer = KaggleRussianNormalizer()\n",
    "\n",
    "    # Load test data\n",
    "    logger.info(\"Loading test data...\")\n",
    "    test_df = pd.read_csv(\"./data/ru_test_2.csv\")\n",
    "\n",
    "    # Create submission\n",
    "    logger.info(\"Creating submission...\")\n",
    "    submission_df = normalizer.create_submission(test_df, \"kaggle_submission.csv\")\n",
    "\n",
    "    logger.info(\"Done!\")\n",
    "\n",
    "    # Test on challenging examples\n",
    "    test_cases = [\n",
    "        # Single digits\n",
    "        \"1\", \"5\", \"0\",\n",
    "\n",
    "        # Simple numbers\n",
    "        \"15\", \"42\", \"101\", \"1984\", \"2023\",\n",
    "\n",
    "        # Dates\n",
    "        \"15.03.2024\", \"1.5.2001\", \"01.01.2000\", \"2023-01-15\",\n",
    "\n",
    "        # Currency\n",
    "        \"25₽\", \"1₽\", \"2₽\", \"5₽\", \"1,5₽\", \"100,50₽\", \"10$\", \"15€\",\n",
    "\n",
    "        # Measurements\n",
    "        \"2,5кг\", \"1м\", \"5км\", \"10°C\", \"37°\", \"15%\",\n",
    "\n",
    "        # Time\n",
    "        \"14:30\", \"08:15\", \"23:59\", \"00:01\", \"12:00:30\",\n",
    "\n",
    "        # Phone numbers\n",
    "        \"+7(495)123-45-67\", \"8 800 555 35 35\",\n",
    "\n",
    "        # Roman numerals\n",
    "        \"I\", \"V\", \"X\", \"XIX\", \"MCMXCIX\",\n",
    "\n",
    "        # Abbreviations\n",
    "        \"РФ\", \"США\", \"ООН\",\n",
    "\n",
    "        # Mixed digit-letter\n",
    "        \"15км\", \"2х\", \"S7\", \"iPhone14\"\n",
    "    ]\n",
    "\n",
    "    logger.info(\"Testing on examples:\")\n",
    "    for test in test_cases:\n",
    "        normalized = normalizer.normalize_text(test)\n",
    "        logger.info(f\"Original: {test}, Normalized: {normalized}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "6cb95b10b69bdac9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 13:31:16,846 - __main__ - INFO - Creating Kaggle-style Russian normalizer...\n",
      "2025-06-01 13:31:16,859 - __main__ - INFO - Loading test data...\n",
      "2025-06-01 13:31:17,125 - __main__ - INFO - Creating submission...\n",
      "2025-06-01 13:31:17,125 - __main__ - INFO - Starting Kaggle-style submission with 989880 rows\n",
      "Normalizing tokens:   0%|          | 4600/989880 [00:00<00:21, 45994.45it/s]2025-06-01 13:31:17,565 - __main__ - INFO - Processed 10000/989880 tokens (1.0%)\n",
      "Normalizing tokens:   1%|▏         | 13097/989880 [00:00<00:14, 68918.89it/s]2025-06-01 13:31:17,682 - __main__ - INFO - Processed 20000/989880 tokens (2.0%)\n",
      "Normalizing tokens:   3%|▎         | 29414/989880 [00:00<00:12, 77129.93it/s]2025-06-01 13:31:17,809 - __main__ - INFO - Processed 30000/989880 tokens (3.0%)\n",
      "Normalizing tokens:   4%|▍         | 37611/989880 [00:00<00:12, 78873.26it/s]2025-06-01 13:31:17,930 - __main__ - INFO - Processed 40000/989880 tokens (4.0%)\n",
      "Normalizing tokens:   5%|▍         | 45961/989880 [00:00<00:11, 80445.45it/s]2025-06-01 13:31:18,019 - __main__ - ERROR - Error normalizing '3/16': '16'\n",
      "2025-06-01 13:31:18,049 - __main__ - INFO - Processed 50000/989880 tokens (5.1%)\n",
      "Normalizing tokens:   5%|▌         | 54394/989880 [00:00<00:11, 81713.21it/s]2025-06-01 13:31:18,168 - __main__ - INFO - Processed 60000/989880 tokens (6.1%)\n",
      "2025-06-01 13:31:18,174 - __main__ - ERROR - Error normalizing '26/27': '27'\n",
      "Normalizing tokens:   6%|▋         | 62759/989880 [00:00<00:11, 82329.23it/s]2025-06-01 13:31:18,288 - __main__ - INFO - Processed 70000/989880 tokens (7.1%)\n",
      "Normalizing tokens:   8%|▊         | 79457/989880 [00:01<00:10, 82956.53it/s]2025-06-01 13:31:18,408 - __main__ - INFO - Processed 80000/989880 tokens (8.1%)\n",
      "Normalizing tokens:   9%|▉         | 88103/989880 [00:01<00:10, 84027.48it/s]2025-06-01 13:31:18,523 - __main__ - INFO - Processed 90000/989880 tokens (9.1%)\n",
      "Normalizing tokens:  10%|▉         | 96696/989880 [00:01<00:10, 84605.15it/s]2025-06-01 13:31:18,639 - __main__ - INFO - Processed 100000/989880 tokens (10.1%)\n",
      "Normalizing tokens:  11%|█         | 105422/989880 [00:01<00:10, 85406.00it/s]2025-06-01 13:31:18,755 - __main__ - INFO - Processed 110000/989880 tokens (11.1%)\n",
      "Normalizing tokens:  12%|█▏        | 114026/989880 [00:01<00:10, 85597.02it/s]2025-06-01 13:31:18,870 - __main__ - INFO - Processed 120000/989880 tokens (12.1%)\n",
      "Normalizing tokens:  12%|█▏        | 122586/989880 [00:01<00:10, 85529.29it/s]2025-06-01 13:31:18,986 - __main__ - INFO - Processed 130000/989880 tokens (13.1%)\n",
      "2025-06-01 13:31:18,988 - __main__ - ERROR - Error normalizing '10/17': '17'\n",
      "Normalizing tokens:  13%|█▎        | 131243/989880 [00:01<00:10, 85842.08it/s]2025-06-01 13:31:19,101 - __main__ - INFO - Processed 140000/989880 tokens (14.1%)\n",
      "Normalizing tokens:  15%|█▌        | 148628/989880 [00:01<00:09, 86273.52it/s]2025-06-01 13:31:19,218 - __main__ - INFO - Processed 150000/989880 tokens (15.2%)\n",
      "Normalizing tokens:  16%|█▌        | 157276/989880 [00:01<00:09, 86332.36it/s]2025-06-01 13:31:19,334 - __main__ - INFO - Processed 160000/989880 tokens (16.2%)\n",
      "Normalizing tokens:  17%|█▋        | 165922/989880 [00:02<00:09, 86368.90it/s]2025-06-01 13:31:19,449 - __main__ - INFO - Processed 170000/989880 tokens (17.2%)\n",
      "Normalizing tokens:  18%|█▊        | 174568/989880 [00:02<00:09, 86395.95it/s]2025-06-01 13:31:19,564 - __main__ - INFO - Processed 180000/989880 tokens (18.2%)\n",
      "Normalizing tokens:  19%|█▊        | 183241/989880 [00:02<00:09, 86495.89it/s]2025-06-01 13:31:19,680 - __main__ - INFO - Processed 190000/989880 tokens (19.2%)\n",
      "Normalizing tokens:  19%|█▉        | 191891/989880 [00:02<00:09, 86479.02it/s]2025-06-01 13:31:19,796 - __main__ - INFO - Processed 200000/989880 tokens (20.2%)\n",
      "2025-06-01 13:31:19,798 - __main__ - ERROR - Error normalizing '75/18': '18'\n",
      "Normalizing tokens:  21%|██        | 209183/989880 [00:02<00:09, 86255.73it/s]2025-06-01 13:31:19,913 - __main__ - INFO - Processed 210000/989880 tokens (21.2%)\n",
      "Normalizing tokens:  22%|██▏       | 217844/989880 [00:02<00:08, 86360.55it/s]2025-06-01 13:31:20,015 - __main__ - ERROR - Error normalizing '4/17': '17'\n",
      "2025-06-01 13:31:20,030 - __main__ - INFO - Processed 220000/989880 tokens (22.2%)\n",
      "Normalizing tokens:  23%|██▎       | 226481/989880 [00:02<00:08, 86209.88it/s]2025-06-01 13:31:20,145 - __main__ - INFO - Processed 230000/989880 tokens (23.2%)\n",
      "Normalizing tokens:  24%|██▍       | 235176/989880 [00:02<00:08, 86427.77it/s]2025-06-01 13:31:20,259 - __main__ - INFO - Processed 240000/989880 tokens (24.2%)\n",
      "2025-06-01 13:31:20,294 - __main__ - ERROR - Error normalizing '10/24': '24'\n",
      "Normalizing tokens:  25%|██▍       | 243819/989880 [00:02<00:08, 86361.73it/s]2025-06-01 13:31:20,375 - __main__ - INFO - Processed 250000/989880 tokens (25.3%)\n",
      "Normalizing tokens:  26%|██▌       | 252461/989880 [00:03<00:08, 86378.90it/s]2025-06-01 13:31:20,405 - __main__ - ERROR - Error normalizing '2017-29-04': '29'\n",
      "2025-06-01 13:31:20,491 - __main__ - INFO - Processed 260000/989880 tokens (26.3%)\n",
      "Normalizing tokens:  26%|██▋       | 261099/989880 [00:03<00:08, 86376.91it/s]2025-06-01 13:31:20,510 - __main__ - ERROR - Error normalizing '33/34': '34'\n",
      "Normalizing tokens:  27%|██▋       | 269791/989880 [00:03<00:08, 86539.17it/s]2025-06-01 13:31:20,607 - __main__ - INFO - Processed 270000/989880 tokens (27.3%)\n",
      "Normalizing tokens:  28%|██▊       | 278497/989880 [00:03<00:08, 86692.69it/s]2025-06-01 13:31:20,722 - __main__ - INFO - Processed 280000/989880 tokens (28.3%)\n",
      "Normalizing tokens:  29%|██▉       | 287167/989880 [00:03<00:08, 86644.24it/s]2025-06-01 13:31:20,837 - __main__ - INFO - Processed 290000/989880 tokens (29.3%)\n",
      "Normalizing tokens:  30%|██▉       | 295848/989880 [00:03<00:08, 86691.48it/s]2025-06-01 13:31:20,952 - __main__ - INFO - Processed 300000/989880 tokens (30.3%)\n",
      "Normalizing tokens:  31%|███       | 304570/989880 [00:03<00:07, 86849.73it/s]2025-06-01 13:31:21,066 - __main__ - INFO - Processed 310000/989880 tokens (31.3%)\n",
      "Normalizing tokens:  32%|███▏      | 313345/989880 [00:03<00:07, 87117.81it/s]2025-06-01 13:31:21,152 - __main__ - ERROR - Error normalizing '2009-25-01': '25'\n",
      "2025-06-01 13:31:21,181 - __main__ - INFO - Processed 320000/989880 tokens (32.3%)\n",
      "Normalizing tokens:  33%|███▎      | 322057/989880 [00:03<00:07, 86826.56it/s]2025-06-01 13:31:21,297 - __main__ - INFO - Processed 330000/989880 tokens (33.3%)\n",
      "Normalizing tokens:  34%|███▍      | 339466/989880 [00:04<00:07, 86927.64it/s]2025-06-01 13:31:21,412 - __main__ - INFO - Processed 340000/989880 tokens (34.3%)\n",
      "Normalizing tokens:  35%|███▌      | 348239/989880 [00:04<00:07, 87165.65it/s]2025-06-01 13:31:21,526 - __main__ - INFO - Processed 350000/989880 tokens (35.4%)\n",
      "Normalizing tokens:  36%|███▌      | 356975/989880 [00:04<00:07, 87222.05it/s]2025-06-01 13:31:21,641 - __main__ - INFO - Processed 360000/989880 tokens (36.4%)\n",
      "Normalizing tokens:  37%|███▋      | 365698/989880 [00:04<00:07, 87163.78it/s]2025-06-01 13:31:21,756 - __main__ - INFO - Processed 370000/989880 tokens (37.4%)\n",
      "Normalizing tokens:  38%|███▊      | 374415/989880 [00:04<00:07, 86684.77it/s]2025-06-01 13:31:21,874 - __main__ - INFO - Processed 380000/989880 tokens (38.4%)\n",
      "Normalizing tokens:  39%|███▊      | 383085/989880 [00:04<00:07, 85775.42it/s]2025-06-01 13:31:21,966 - __main__ - ERROR - Error normalizing '11/19': '19'\n",
      "2025-06-01 13:31:21,990 - __main__ - INFO - Processed 390000/989880 tokens (39.4%)\n",
      "Normalizing tokens:  40%|███▉      | 391757/989880 [00:04<00:06, 86054.72it/s]2025-06-01 13:31:22,106 - __main__ - INFO - Processed 400000/989880 tokens (40.4%)\n",
      "Normalizing tokens:  41%|████▏     | 409196/989880 [00:04<00:06, 86665.15it/s]2025-06-01 13:31:22,221 - __main__ - INFO - Processed 410000/989880 tokens (41.4%)\n",
      "Normalizing tokens:  42%|████▏     | 417997/989880 [00:04<00:06, 87066.65it/s]2025-06-01 13:31:22,334 - __main__ - INFO - Processed 420000/989880 tokens (42.4%)\n",
      "2025-06-01 13:31:22,368 - __main__ - ERROR - Error normalizing '40/40': '40'\n",
      "2025-06-01 13:31:22,401 - __main__ - ERROR - Error normalizing '2009-13-15': '13'\n",
      "Normalizing tokens:  43%|████▎     | 426705/989880 [00:05<00:06, 86846.57it/s]2025-06-01 13:31:22,450 - __main__ - INFO - Processed 430000/989880 tokens (43.4%)\n",
      "Normalizing tokens:  44%|████▍     | 435391/989880 [00:05<00:06, 85805.41it/s]2025-06-01 13:31:22,568 - __main__ - INFO - Processed 440000/989880 tokens (44.4%)\n",
      "Normalizing tokens:  45%|████▍     | 444190/989880 [00:05<00:06, 86452.89it/s]2025-06-01 13:31:22,682 - __main__ - INFO - Processed 450000/989880 tokens (45.5%)\n",
      "Normalizing tokens:  46%|████▌     | 453011/989880 [00:05<00:06, 86974.07it/s]2025-06-01 13:31:22,796 - __main__ - INFO - Processed 460000/989880 tokens (46.5%)\n",
      "Normalizing tokens:  47%|████▋     | 461755/989880 [00:05<00:06, 87110.02it/s]2025-06-01 13:31:22,910 - __main__ - INFO - Processed 470000/989880 tokens (47.5%)\n",
      "Normalizing tokens:  48%|████▊     | 479374/989880 [00:05<00:05, 87669.44it/s]2025-06-01 13:31:23,023 - __main__ - INFO - Processed 480000/989880 tokens (48.5%)\n",
      "Normalizing tokens:  49%|████▉     | 488241/989880 [00:05<00:05, 87968.50it/s]2025-06-01 13:31:23,136 - __main__ - INFO - Processed 490000/989880 tokens (49.5%)\n",
      "Normalizing tokens:  50%|█████     | 497095/989880 [00:05<00:05, 88136.85it/s]2025-06-01 13:31:23,249 - __main__ - INFO - Processed 500000/989880 tokens (50.5%)\n",
      "Normalizing tokens:  51%|█████     | 505939/989880 [00:05<00:05, 88227.16it/s]2025-06-01 13:31:23,362 - __main__ - INFO - Processed 510000/989880 tokens (51.5%)\n",
      "Normalizing tokens:  52%|█████▏    | 514777/989880 [00:06<00:05, 88270.61it/s]2025-06-01 13:31:23,476 - __main__ - INFO - Processed 520000/989880 tokens (52.5%)\n",
      "Normalizing tokens:  53%|█████▎    | 523624/989880 [00:06<00:05, 88327.77it/s]2025-06-01 13:31:23,589 - __main__ - INFO - Processed 530000/989880 tokens (53.5%)\n",
      "Normalizing tokens:  54%|█████▍    | 532457/989880 [00:06<00:05, 87962.30it/s]2025-06-01 13:31:23,703 - __main__ - INFO - Processed 540000/989880 tokens (54.6%)\n",
      "Normalizing tokens:  55%|█████▍    | 541309/989880 [00:06<00:05, 88128.23it/s]2025-06-01 13:31:23,817 - __main__ - INFO - Processed 550000/989880 tokens (55.6%)\n",
      "Normalizing tokens:  56%|█████▌    | 550123/989880 [00:06<00:05, 87923.64it/s]2025-06-01 13:31:23,881 - __main__ - ERROR - Error normalizing '98/37': '37'\n",
      "2025-06-01 13:31:23,910 - __main__ - ERROR - Error normalizing '2007-29-03': '29'\n",
      "Normalizing tokens:  56%|█████▋    | 558916/989880 [00:06<00:04, 87677.71it/s]2025-06-01 13:31:23,932 - __main__ - INFO - Processed 560000/989880 tokens (56.6%)\n",
      "2025-06-01 13:31:23,956 - __main__ - ERROR - Error normalizing '40/19': '19'\n",
      "Normalizing tokens:  57%|█████▋    | 567685/989880 [00:06<00:04, 87195.57it/s]2025-06-01 13:31:24,048 - __main__ - INFO - Processed 570000/989880 tokens (57.6%)\n",
      "Normalizing tokens:  58%|█████▊    | 576406/989880 [00:06<00:04, 87197.54it/s]2025-06-01 13:31:24,163 - __main__ - INFO - Processed 580000/989880 tokens (58.6%)\n",
      "Normalizing tokens:  59%|█████▉    | 585127/989880 [00:06<00:04, 87197.13it/s]2025-06-01 13:31:24,277 - __main__ - INFO - Processed 590000/989880 tokens (59.6%)\n",
      "Normalizing tokens:  60%|█████▉    | 593886/989880 [00:06<00:04, 87312.18it/s]2025-06-01 13:31:24,391 - __main__ - INFO - Processed 600000/989880 tokens (60.6%)\n",
      "Normalizing tokens:  61%|██████    | 602649/989880 [00:07<00:04, 87404.71it/s]2025-06-01 13:31:24,510 - __main__ - INFO - Processed 610000/989880 tokens (61.6%)\n",
      "Normalizing tokens:  62%|██████▏   | 611390/989880 [00:07<00:04, 86087.31it/s]2025-06-01 13:31:24,592 - __main__ - ERROR - Error normalizing '2003-20-10': '20'\n",
      "2025-06-01 13:31:24,625 - __main__ - INFO - Processed 620000/989880 tokens (62.6%)\n",
      "Normalizing tokens:  63%|██████▎   | 620060/989880 [00:07<00:04, 86267.38it/s]2025-06-01 13:31:24,717 - __main__ - ERROR - Error normalizing '9/28': '28'\n",
      "2025-06-01 13:31:24,720 - __main__ - ERROR - Error normalizing '1730е': name 'cardinal_parts' is not defined\n",
      "Normalizing tokens:  64%|██████▎   | 628781/989880 [00:07<00:04, 86546.80it/s]2025-06-01 13:31:24,741 - __main__ - INFO - Processed 630000/989880 tokens (63.6%)\n",
      "2025-06-01 13:31:24,792 - __main__ - ERROR - Error normalizing '2016-28-12': '28'\n",
      "Normalizing tokens:  64%|██████▍   | 637439/989880 [00:07<00:04, 86386.14it/s]2025-06-01 13:31:24,856 - __main__ - INFO - Processed 640000/989880 tokens (64.7%)\n",
      "Normalizing tokens:  65%|██████▌   | 646122/989880 [00:07<00:03, 86517.06it/s]2025-06-01 13:31:24,972 - __main__ - INFO - Processed 650000/989880 tokens (65.7%)\n",
      "Normalizing tokens:  66%|██████▌   | 654776/989880 [00:07<00:03, 86452.55it/s]2025-06-01 13:31:25,091 - __main__ - INFO - Processed 660000/989880 tokens (66.7%)\n",
      "Normalizing tokens:  67%|██████▋   | 663423/989880 [00:07<00:03, 85912.37it/s]2025-06-01 13:31:25,206 - __main__ - INFO - Processed 670000/989880 tokens (67.7%)\n",
      "Normalizing tokens:  68%|██████▊   | 672041/989880 [00:07<00:03, 85990.50it/s]2025-06-01 13:31:25,321 - __main__ - INFO - Processed 680000/989880 tokens (68.7%)\n",
      "Normalizing tokens:  69%|██████▉   | 680702/989880 [00:07<00:03, 86173.17it/s]2025-06-01 13:31:25,333 - __main__ - ERROR - Error normalizing '3/33': '33'\n",
      "2025-06-01 13:31:25,333 - __main__ - ERROR - Error normalizing '3/35': '35'\n",
      "Normalizing tokens:  70%|██████▉   | 689321/989880 [00:08<00:03, 86014.44it/s]2025-06-01 13:31:25,439 - __main__ - INFO - Processed 690000/989880 tokens (69.7%)\n",
      "Normalizing tokens:  71%|███████   | 698120/989880 [00:08<00:03, 86604.41it/s]2025-06-01 13:31:25,552 - __main__ - INFO - Processed 700000/989880 tokens (70.7%)\n",
      "Normalizing tokens:  71%|███████▏  | 706888/989880 [00:08<00:03, 86924.92it/s]2025-06-01 13:31:25,665 - __main__ - INFO - Processed 710000/989880 tokens (71.7%)\n",
      "Normalizing tokens:  72%|███████▏  | 715685/989880 [00:08<00:03, 87237.40it/s]2025-06-01 13:31:25,779 - __main__ - INFO - Processed 720000/989880 tokens (72.7%)\n",
      "Normalizing tokens:  73%|███████▎  | 724421/989880 [00:08<00:03, 87271.73it/s]2025-06-01 13:31:25,893 - __main__ - INFO - Processed 730000/989880 tokens (73.7%)\n",
      "Normalizing tokens:  74%|███████▍  | 733235/989880 [00:08<00:02, 87530.31it/s]2025-06-01 13:31:26,007 - __main__ - INFO - Processed 740000/989880 tokens (74.8%)\n",
      "Normalizing tokens:  75%|███████▍  | 742029/989880 [00:08<00:02, 87649.97it/s]2025-06-01 13:31:26,121 - __main__ - INFO - Processed 750000/989880 tokens (75.8%)\n",
      "Normalizing tokens:  77%|███████▋  | 759618/989880 [00:08<00:02, 87821.40it/s]2025-06-01 13:31:26,235 - __main__ - INFO - Processed 760000/989880 tokens (76.8%)\n",
      "Normalizing tokens:  78%|███████▊  | 768425/989880 [00:08<00:02, 87894.67it/s]2025-06-01 13:31:26,348 - __main__ - INFO - Processed 770000/989880 tokens (77.8%)\n",
      "Normalizing tokens:  79%|███████▊  | 777269/989880 [00:09<00:02, 88056.12it/s]2025-06-01 13:31:26,461 - __main__ - INFO - Processed 780000/989880 tokens (78.8%)\n",
      "Normalizing tokens:  79%|███████▉  | 786085/989880 [00:09<00:02, 88084.91it/s]2025-06-01 13:31:26,574 - __main__ - INFO - Processed 790000/989880 tokens (79.8%)\n",
      "Normalizing tokens:  80%|████████  | 794906/989880 [00:09<00:02, 88119.75it/s]2025-06-01 13:31:26,688 - __main__ - INFO - Processed 800000/989880 tokens (80.8%)\n",
      "Normalizing tokens:  81%|████████  | 803718/989880 [00:09<00:02, 87959.69it/s]2025-06-01 13:31:26,805 - __main__ - INFO - Processed 810000/989880 tokens (81.8%)\n",
      "Normalizing tokens:  82%|████████▏ | 812515/989880 [00:09<00:02, 87304.46it/s]2025-06-01 13:31:26,905 - __main__ - ERROR - Error normalizing '2013-13-13': '13'\n",
      "2025-06-01 13:31:26,919 - __main__ - INFO - Processed 820000/989880 tokens (82.8%)\n",
      "Normalizing tokens:  83%|████████▎ | 821247/989880 [00:09<00:01, 87050.84it/s]2025-06-01 13:31:27,034 - __main__ - INFO - Processed 830000/989880 tokens (83.8%)\n",
      "Normalizing tokens:  84%|████████▍ | 830000/989880 [00:09<00:01, 87189.79it/s]2025-06-01 13:31:27,129 - __main__ - ERROR - Error normalizing '18/36': '36'\n",
      "Normalizing tokens:  85%|████████▍ | 838751/989880 [00:09<00:01, 87283.27it/s]2025-06-01 13:31:27,149 - __main__ - INFO - Processed 840000/989880 tokens (84.9%)\n",
      "2025-06-01 13:31:27,220 - __main__ - ERROR - Error normalizing '1/92': '92'\n",
      "Normalizing tokens:  86%|████████▌ | 847480/989880 [00:09<00:01, 87113.07it/s]2025-06-01 13:31:27,264 - __main__ - INFO - Processed 850000/989880 tokens (85.9%)\n",
      "Normalizing tokens:  86%|████████▋ | 856226/989880 [00:09<00:01, 87213.63it/s]2025-06-01 13:31:27,379 - __main__ - INFO - Processed 860000/989880 tokens (86.9%)\n",
      "Normalizing tokens:  87%|████████▋ | 864948/989880 [00:10<00:01, 87068.16it/s]2025-06-01 13:31:27,493 - __main__ - INFO - Processed 870000/989880 tokens (87.9%)\n",
      "Normalizing tokens:  88%|████████▊ | 873715/989880 [00:10<00:01, 87247.64it/s]2025-06-01 13:31:27,608 - __main__ - INFO - Processed 880000/989880 tokens (88.9%)\n",
      "Normalizing tokens:  89%|████████▉ | 882454/989880 [00:10<00:01, 87287.92it/s]2025-06-01 13:31:27,722 - __main__ - INFO - Processed 890000/989880 tokens (89.9%)\n",
      "Normalizing tokens:  91%|█████████ | 899967/989880 [00:10<00:01, 87446.26it/s]2025-06-01 13:31:27,836 - __main__ - INFO - Processed 900000/989880 tokens (90.9%)\n",
      "Normalizing tokens:  92%|█████████▏| 908812/989880 [00:10<00:00, 87745.49it/s]2025-06-01 13:31:27,950 - __main__ - INFO - Processed 910000/989880 tokens (91.9%)\n",
      "2025-06-01 13:31:27,998 - __main__ - ERROR - Error normalizing '19/22': '22'\n",
      "Normalizing tokens:  93%|█████████▎| 917587/989880 [00:10<00:00, 87428.37it/s]2025-06-01 13:31:28,065 - __main__ - INFO - Processed 920000/989880 tokens (92.9%)\n",
      "2025-06-01 13:31:28,082 - __main__ - ERROR - Error normalizing '1/32': '32'\n",
      "Normalizing tokens:  94%|█████████▎| 926331/989880 [00:10<00:00, 87165.13it/s]2025-06-01 13:31:28,180 - __main__ - INFO - Processed 930000/989880 tokens (94.0%)\n",
      "Normalizing tokens:  94%|█████████▍| 935060/989880 [00:10<00:00, 87201.59it/s]2025-06-01 13:31:28,295 - __main__ - INFO - Processed 940000/989880 tokens (95.0%)\n",
      "Normalizing tokens:  95%|█████████▌| 943846/989880 [00:10<00:00, 87396.13it/s]2025-06-01 13:31:28,408 - __main__ - INFO - Processed 950000/989880 tokens (96.0%)\n",
      "Normalizing tokens:  96%|█████████▌| 952640/989880 [00:11<00:00, 87556.60it/s]2025-06-01 13:31:28,445 - __main__ - ERROR - Error normalizing '1/16': '16'\n",
      "2025-06-01 13:31:28,523 - __main__ - INFO - Processed 960000/989880 tokens (97.0%)\n",
      "Normalizing tokens:  97%|█████████▋| 961396/989880 [00:11<00:00, 87335.28it/s]2025-06-01 13:31:28,636 - __main__ - INFO - Processed 970000/989880 tokens (98.0%)\n",
      "Normalizing tokens:  99%|█████████▉| 979079/989880 [00:11<00:00, 87895.35it/s]2025-06-01 13:31:28,750 - __main__ - INFO - Processed 980000/989880 tokens (99.0%)\n",
      "Normalizing tokens: 100%|██████████| 989880/989880 [00:11<00:00, 86348.96it/s]\n",
      "2025-06-01 13:31:29,454 - __main__ - INFO - Kaggle submission saved to kaggle_submission.csv\n",
      "2025-06-01 13:31:29,514 - __main__ - INFO - Done!\n",
      "2025-06-01 13:31:29,517 - __main__ - INFO - Testing on examples:\n",
      "2025-06-01 13:31:29,518 - __main__ - INFO - Original: 1, Normalized: один\n",
      "2025-06-01 13:31:29,518 - __main__ - INFO - Original: 5, Normalized: пять\n",
      "2025-06-01 13:31:29,519 - __main__ - INFO - Original: 0, Normalized: ноль\n",
      "2025-06-01 13:31:29,519 - __main__ - INFO - Original: 15, Normalized: пятнадцать\n",
      "2025-06-01 13:31:29,520 - __main__ - INFO - Original: 42, Normalized: сорок два\n",
      "2025-06-01 13:31:29,521 - __main__ - INFO - Original: 101, Normalized: сто один\n",
      "2025-06-01 13:31:29,522 - __main__ - INFO - Original: 1984, Normalized: одна тысяча девятьсот восемьдесят четыре\n",
      "2025-06-01 13:31:29,523 - __main__ - INFO - Original: 2023, Normalized: две тысячи двадцать три\n",
      "2025-06-01 13:31:29,524 - __main__ - INFO - Original: 15.03.2024, Normalized: пятнадцатое марта две тысячи двадцать четвертого года\n",
      "2025-06-01 13:31:29,525 - __main__ - INFO - Original: 1.5.2001, Normalized: первое мая две тысячи первого года\n",
      "2025-06-01 13:31:29,525 - __main__ - INFO - Original: 01.01.2000, Normalized: первое января двухтысячного года\n",
      "2025-06-01 13:31:29,526 - __main__ - INFO - Original: 2023-01-15, Normalized: пятнадцатое января две тысячи двадцать третьего года\n",
      "2025-06-01 13:31:29,526 - __main__ - INFO - Original: 25₽, Normalized: двадцать пять рублей\n",
      "2025-06-01 13:31:29,527 - __main__ - INFO - Original: 1₽, Normalized: один рубль\n",
      "2025-06-01 13:31:29,527 - __main__ - INFO - Original: 2₽, Normalized: два рубля\n",
      "2025-06-01 13:31:29,527 - __main__ - INFO - Original: 5₽, Normalized: пять рублей\n",
      "2025-06-01 13:31:29,528 - __main__ - INFO - Original: 1,5₽, Normalized: один рубль пять копеек\n",
      "2025-06-01 13:31:29,529 - __main__ - INFO - Original: 100,50₽, Normalized: сто рублей пятьдесят копеек\n",
      "2025-06-01 13:31:29,530 - __main__ - INFO - Original: 10$, Normalized: десять долларов\n",
      "2025-06-01 13:31:29,530 - __main__ - INFO - Original: 15€, Normalized: пятнадцать евро\n",
      "2025-06-01 13:31:29,531 - __main__ - INFO - Original: 2,5кг, Normalized: два целых пять десятых килограмма\n",
      "2025-06-01 13:31:29,531 - __main__ - INFO - Original: 1м, Normalized: один метр\n",
      "2025-06-01 13:31:29,531 - __main__ - INFO - Original: 5км, Normalized: пять километров\n",
      "2025-06-01 13:31:29,532 - __main__ - INFO - Original: 10°C, Normalized: десять градусов цельсия\n",
      "2025-06-01 13:31:29,532 - __main__ - INFO - Original: 37°, Normalized: тридцать семь градусов\n",
      "2025-06-01 13:31:29,533 - __main__ - INFO - Original: 15%, Normalized: пятнадцать процентов\n",
      "2025-06-01 13:31:29,533 - __main__ - INFO - Original: 14:30, Normalized: четырнадцать часов тридцать минут\n",
      "2025-06-01 13:31:29,534 - __main__ - INFO - Original: 08:15, Normalized: восемь часов пятнадцать минут\n",
      "2025-06-01 13:31:29,534 - __main__ - INFO - Original: 23:59, Normalized: двадцать три часа пятьдесят девять минут\n",
      "2025-06-01 13:31:29,535 - __main__ - INFO - Original: 00:01, Normalized: ноль часов один минута\n",
      "2025-06-01 13:31:29,536 - __main__ - INFO - Original: 12:00:30, Normalized: двенадцать часов тридцать секунд\n",
      "2025-06-01 13:31:29,537 - __main__ - INFO - Original: +7(495)123-45-67, Normalized: плюс семь четыре девять пять один два три четыре пять шесть семь\n",
      "2025-06-01 13:31:29,537 - __main__ - INFO - Original: 8 800 555 35 35, Normalized: восемь восемь ноль ноль пять пять пять три пять три пять\n",
      "2025-06-01 13:31:29,538 - __main__ - INFO - Original: I, Normalized: один\n",
      "2025-06-01 13:31:29,539 - __main__ - INFO - Original: V, Normalized: пять\n",
      "2025-06-01 13:31:29,539 - __main__ - INFO - Original: X, Normalized: десять\n",
      "2025-06-01 13:31:29,539 - __main__ - INFO - Original: XIX, Normalized: девятнадцать\n",
      "2025-06-01 13:31:29,540 - __main__ - INFO - Original: MCMXCIX, Normalized: одна тысяча девятьсот девяносто девять\n",
      "2025-06-01 13:31:29,541 - __main__ - INFO - Original: РФ, Normalized: российская федерация\n",
      "2025-06-01 13:31:29,541 - __main__ - INFO - Original: США, Normalized: соединенные штаты америки\n",
      "2025-06-01 13:31:29,541 - __main__ - INFO - Original: ООН, Normalized: организация объединенных наций\n",
      "2025-06-01 13:31:29,542 - __main__ - INFO - Original: 15км, Normalized: пятнадцать километров\n",
      "2025-06-01 13:31:29,543 - __main__ - INFO - Original: 2х, Normalized: два х\n",
      "2025-06-01 13:31:29,543 - __main__ - INFO - Original: S7, Normalized: s семь\n",
      "2025-06-01 13:31:29,543 - __main__ - INFO - Original: iPhone14, Normalized: iphone четырнадцать\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
